{
  
    
        "post0": {
            "title": "Telecom Churn Prediction",
            "content": "import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns . train = pd.read_csv(&#39;C: Users HP Downloads archive (98) cell2celltrain.csv&#39;) . test = pd.read_csv(&#39;C: Users HP Downloads archive (98) cell2cellholdout.csv&#39;) . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 51047 entries, 0 to 51046 Data columns (total 58 columns): # Column Non-Null Count Dtype -- -- 0 CustomerID 51047 non-null int64 1 Churn 51047 non-null object 2 MonthlyRevenue 50891 non-null float64 3 MonthlyMinutes 50891 non-null float64 4 TotalRecurringCharge 50891 non-null float64 5 DirectorAssistedCalls 50891 non-null float64 6 OverageMinutes 50891 non-null float64 7 RoamingCalls 50891 non-null float64 8 PercChangeMinutes 50680 non-null float64 9 PercChangeRevenues 50680 non-null float64 10 DroppedCalls 51047 non-null float64 11 BlockedCalls 51047 non-null float64 12 UnansweredCalls 51047 non-null float64 13 CustomerCareCalls 51047 non-null float64 14 ThreewayCalls 51047 non-null float64 15 ReceivedCalls 51047 non-null float64 16 OutboundCalls 51047 non-null float64 17 InboundCalls 51047 non-null float64 18 PeakCallsInOut 51047 non-null float64 19 OffPeakCallsInOut 51047 non-null float64 20 DroppedBlockedCalls 51047 non-null float64 21 CallForwardingCalls 51047 non-null float64 22 CallWaitingCalls 51047 non-null float64 23 MonthsInService 51047 non-null int64 24 UniqueSubs 51047 non-null int64 25 ActiveSubs 51047 non-null int64 26 ServiceArea 51023 non-null object 27 Handsets 51046 non-null float64 28 HandsetModels 51046 non-null float64 29 CurrentEquipmentDays 51046 non-null float64 30 AgeHH1 50138 non-null float64 31 AgeHH2 50138 non-null float64 32 ChildrenInHH 51047 non-null object 33 HandsetRefurbished 51047 non-null object 34 HandsetWebCapable 51047 non-null object 35 TruckOwner 51047 non-null object 36 RVOwner 51047 non-null object 37 Homeownership 51047 non-null object 38 BuysViaMailOrder 51047 non-null object 39 RespondsToMailOffers 51047 non-null object 40 OptOutMailings 51047 non-null object 41 NonUSTravel 51047 non-null object 42 OwnsComputer 51047 non-null object 43 HasCreditCard 51047 non-null object 44 RetentionCalls 51047 non-null int64 45 RetentionOffersAccepted 51047 non-null int64 46 NewCellphoneUser 51047 non-null object 47 NotNewCellphoneUser 51047 non-null object 48 ReferralsMadeBySubscriber 51047 non-null int64 49 IncomeGroup 51047 non-null int64 50 OwnsMotorcycle 51047 non-null object 51 AdjustmentsToCreditRating 51047 non-null int64 52 HandsetPrice 51047 non-null object 53 MadeCallToRetentionTeam 51047 non-null object 54 CreditRating 51047 non-null object 55 PrizmCode 51047 non-null object 56 Occupation 51047 non-null object 57 MaritalStatus 51047 non-null object dtypes: float64(26), int64(9), object(23) memory usage: 22.6+ MB . test.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 20000 entries, 0 to 19999 Data columns (total 58 columns): # Column Non-Null Count Dtype -- -- 0 CustomerID 20000 non-null int64 1 Churn 0 non-null float64 2 MonthlyRevenue 19940 non-null float64 3 MonthlyMinutes 19940 non-null float64 4 TotalRecurringCharge 19940 non-null float64 5 DirectorAssistedCalls 19940 non-null float64 6 OverageMinutes 19940 non-null float64 7 RoamingCalls 19940 non-null float64 8 PercChangeMinutes 19865 non-null float64 9 PercChangeRevenues 19865 non-null float64 10 DroppedCalls 20000 non-null float64 11 BlockedCalls 20000 non-null float64 12 UnansweredCalls 20000 non-null float64 13 CustomerCareCalls 20000 non-null float64 14 ThreewayCalls 20000 non-null float64 15 ReceivedCalls 20000 non-null float64 16 OutboundCalls 20000 non-null float64 17 InboundCalls 20000 non-null float64 18 PeakCallsInOut 20000 non-null float64 19 OffPeakCallsInOut 20000 non-null float64 20 DroppedBlockedCalls 20000 non-null float64 21 CallForwardingCalls 20000 non-null float64 22 CallWaitingCalls 20000 non-null float64 23 MonthsInService 20000 non-null int64 24 UniqueSubs 20000 non-null int64 25 ActiveSubs 20000 non-null int64 26 ServiceArea 19996 non-null object 27 Handsets 20000 non-null int64 28 HandsetModels 20000 non-null int64 29 CurrentEquipmentDays 20000 non-null int64 30 AgeHH1 19665 non-null float64 31 AgeHH2 19665 non-null float64 32 ChildrenInHH 20000 non-null object 33 HandsetRefurbished 20000 non-null object 34 HandsetWebCapable 20000 non-null object 35 TruckOwner 20000 non-null object 36 RVOwner 20000 non-null object 37 Homeownership 20000 non-null object 38 BuysViaMailOrder 20000 non-null object 39 RespondsToMailOffers 20000 non-null object 40 OptOutMailings 20000 non-null object 41 NonUSTravel 20000 non-null object 42 OwnsComputer 20000 non-null object 43 HasCreditCard 20000 non-null object 44 RetentionCalls 20000 non-null int64 45 RetentionOffersAccepted 20000 non-null int64 46 NewCellphoneUser 20000 non-null object 47 NotNewCellphoneUser 20000 non-null object 48 ReferralsMadeBySubscriber 20000 non-null int64 49 IncomeGroup 20000 non-null int64 50 OwnsMotorcycle 20000 non-null object 51 AdjustmentsToCreditRating 20000 non-null int64 52 HandsetPrice 20000 non-null object 53 MadeCallToRetentionTeam 20000 non-null object 54 CreditRating 20000 non-null object 55 PrizmCode 20000 non-null object 56 Occupation 20000 non-null object 57 MaritalStatus 20000 non-null object dtypes: float64(24), int64(12), object(22) memory usage: 8.9+ MB . numeric = train.select_dtypes(include=[np.number]) . categorical = train.select_dtypes(exclude=[np.number]) . categorical . Churn ServiceArea ChildrenInHH HandsetRefurbished HandsetWebCapable TruckOwner RVOwner Homeownership BuysViaMailOrder RespondsToMailOffers ... HasCreditCard NewCellphoneUser NotNewCellphoneUser OwnsMotorcycle HandsetPrice MadeCallToRetentionTeam CreditRating PrizmCode Occupation MaritalStatus . 0 Yes | SEAPOR503 | No | No | Yes | No | No | Known | Yes | Yes | ... | Yes | No | No | No | 30 | Yes | 1-Highest | Suburban | Professional | No | . 1 Yes | PITHOM412 | Yes | No | No | No | No | Known | Yes | Yes | ... | Yes | Yes | No | No | 30 | No | 4-Medium | Suburban | Professional | Yes | . 2 No | MILMIL414 | Yes | No | No | No | No | Unknown | No | No | ... | Yes | Yes | No | No | Unknown | No | 3-Good | Town | Crafts | Yes | . 3 No | PITHOM412 | No | No | Yes | No | No | Known | Yes | Yes | ... | Yes | Yes | No | No | 10 | No | 4-Medium | Other | Other | No | . 4 Yes | OKCTUL918 | No | No | No | No | No | Known | Yes | Yes | ... | Yes | No | Yes | No | 10 | No | 1-Highest | Other | Professional | Yes | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 51042 Yes | LAXSFN818 | No | Yes | Yes | No | No | Known | Yes | Yes | ... | Yes | No | No | No | 60 | No | 1-Highest | Suburban | Other | Yes | . 51043 No | LAXCDG310 | Yes | No | Yes | No | No | Known | Yes | Yes | ... | Yes | No | No | No | 60 | No | 3-Good | Other | Other | No | . 51044 Yes | LAXCDG310 | No | No | Yes | No | No | Known | No | No | ... | Yes | No | No | No | 80 | No | 5-Low | Other | Clerical | No | . 51045 No | NEVPOW619 | Yes | No | Yes | No | No | Unknown | No | No | ... | No | No | No | No | 30 | No | 5-Low | Other | Other | No | . 51046 No | NEVPOW619 | No | No | Yes | No | No | Unknown | No | No | ... | No | No | No | No | 60 | Yes | 5-Low | Other | Other | Unknown | . 51047 rows × 23 columns . numeric . CustomerID MonthlyRevenue MonthlyMinutes TotalRecurringCharge DirectorAssistedCalls OverageMinutes RoamingCalls PercChangeMinutes PercChangeRevenues DroppedCalls ... Handsets HandsetModels CurrentEquipmentDays AgeHH1 AgeHH2 RetentionCalls RetentionOffersAccepted ReferralsMadeBySubscriber IncomeGroup AdjustmentsToCreditRating . 0 3000002 | 24.00 | 219.0 | 22.0 | 0.25 | 0.0 | 0.0 | -157.0 | -19.0 | 0.7 | ... | 2.0 | 2.0 | 361.0 | 62.0 | 0.0 | 1 | 0 | 0 | 4 | 0 | . 1 3000010 | 16.99 | 10.0 | 17.0 | 0.00 | 0.0 | 0.0 | -4.0 | 0.0 | 0.3 | ... | 2.0 | 1.0 | 1504.0 | 40.0 | 42.0 | 0 | 0 | 0 | 5 | 0 | . 2 3000014 | 38.00 | 8.0 | 38.0 | 0.00 | 0.0 | 0.0 | -2.0 | 0.0 | 0.0 | ... | 1.0 | 1.0 | 1812.0 | 26.0 | 26.0 | 0 | 0 | 0 | 6 | 0 | . 3 3000022 | 82.28 | 1312.0 | 75.0 | 1.24 | 0.0 | 0.0 | 157.0 | 8.1 | 52.0 | ... | 9.0 | 4.0 | 458.0 | 30.0 | 0.0 | 0 | 0 | 0 | 6 | 0 | . 4 3000026 | 17.14 | 0.0 | 17.0 | 0.00 | 0.0 | 0.0 | 0.0 | -0.2 | 0.0 | ... | 4.0 | 3.0 | 852.0 | 46.0 | 54.0 | 0 | 0 | 0 | 9 | 1 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 51042 3399958 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 9.3 | ... | 2.0 | 2.0 | 526.0 | 68.0 | 64.0 | 0 | 0 | 0 | 6 | 0 | . 51043 3399974 | 95.17 | 1745.0 | 85.0 | 0.99 | 45.0 | 4.7 | 122.0 | 15.9 | 16.7 | ... | 2.0 | 2.0 | 464.0 | 48.0 | 48.0 | 0 | 0 | 0 | 9 | 1 | . 51044 3399978 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 71.7 | ... | 3.0 | 2.0 | 378.0 | 36.0 | 0.0 | 0 | 0 | 0 | 7 | 1 | . 51045 3399990 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 0.0 | ... | 2.0 | 2.0 | 433.0 | 32.0 | 0.0 | 0 | 0 | 0 | 9 | 0 | . 51046 3399994 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 18.7 | ... | 7.0 | 5.0 | 75.0 | 0.0 | 0.0 | 2 | 1 | 0 | 0 | 1 | . 51047 rows × 35 columns . EDA . plt.figure(figsize=(8,4),dpi=120) sns.scatterplot(data=train,y=&#39;MonthlyRevenue&#39;,x=&#39;MonthlyMinutes&#39;,alpha=0.7) . &lt;AxesSubplot:xlabel=&#39;MonthlyMinutes&#39;, ylabel=&#39;MonthlyRevenue&#39;&gt; . plt.figure(figsize=(8,4),dpi=120) sns.scatterplot(data=train,x=&#39;MonthlyMinutes&#39;,y=&#39;MonthlyRevenue&#39;,hue=&#39;Churn&#39;,alpha=0.7) . &lt;AxesSubplot:xlabel=&#39;MonthlyMinutes&#39;, ylabel=&#39;MonthlyRevenue&#39;&gt; . plt.figure(figsize=(8,4),dpi=120) sns.scatterplot(data=train,x=&#39;MonthlyMinutes&#39;,y=&#39;MonthlyRevenue&#39;,hue=&#39;Churn&#39;,alpha=0.7) plt.xlim(3000) . (3000.0, 7726.95) . Customers with more monthly minutes are less likely to churn out. . plt.figure(figsize=(8,4),dpi=120) sns.histplot(data=test,x=&#39;MonthsInService&#39;,kde=&#39;True&#39;) . &lt;AxesSubplot:xlabel=&#39;MonthsInService&#39;, ylabel=&#39;Count&#39;&gt; . plt.figure(figsize=(8,4),dpi=120) sns.boxplot(data=train,x=&#39;MonthsInService&#39;) . &lt;AxesSubplot:xlabel=&#39;MonthsInService&#39;&gt; . def duration(months): if months &lt;= 10: return &#39;Less than one year&#39; elif months &gt;10 and months &lt;=20: return &#39;Less than two years&#39; elif months &gt;20 and months &lt;= 30: return &#39;Less than three years&#39; elif months &gt; 30 and months &lt;= 40: return &#39;Less than four years&#39; elif months &gt; 40 and months &lt;= 50: return &#39;Less than five years&#39; elif months &gt; 50 and months &lt;= 60: return &#39;Less than six years&#39; elif months &gt; 60 and months &lt;= 70: return &#39;Less then seven years&#39; train[&#39;Subscription_months&#39;] = train[&#39;MonthsInService&#39;].apply(duration) test[&#39;Subscription_months&#39;] = test[&#39;MonthsInService&#39;].apply(duration) . train[&#39;Subscription_months&#39;].value_counts().sort_values() . Less then seven years 1 Less than six years 387 Less than five years 1365 Less than four years 4989 Less than one year 10639 Less than three years 11996 Less than two years 21670 Name: Subscription_months, dtype: int64 . train[&#39;Occupation&#39;].value_counts() . Other 37637 Professional 8755 Crafts 1519 Clerical 986 Self 879 Retired 733 Student 381 Homemaker 157 Name: Occupation, dtype: int64 . plt.figure(figsize=(8,4),dpi=120) sns.countplot(data=train,x=&#39;Occupation&#39;) . &lt;AxesSubplot:xlabel=&#39;Occupation&#39;, ylabel=&#39;count&#39;&gt; . plt.figure(figsize=(8,4),dpi=120) sns.barplot(data=train,x=&#39;Occupation&#39;,y=&#39;MonthlyRevenue&#39;,estimator=np.mean) . &lt;AxesSubplot:xlabel=&#39;Occupation&#39;, ylabel=&#39;MonthlyRevenue&#39;&gt; . Lowest revenue is generated from Retired category, and other categories have a fair distribution in generating revenue. . plt.figure(figsize=(8,6),dpi=120) sns.barplot(data=train,x=&#39;Subscription_months&#39;,y=&#39;MonthlyRevenue&#39;,estimator=np.mean) plt.xticks(rotation=90); . Fair distribution of monthly revenue generated. . train[&#39;CreditRating&#39;].value_counts().sort_values() . 6-VeryLow 1152 7-Lowest 2114 4-Medium 5357 5-Low 6499 3-Good 8410 1-Highest 8522 2-High 18993 Name: CreditRating, dtype: int64 . train[&#39;IncomeGroup&#39;].value_counts() . 0 12835 6 9607 7 5877 9 5563 5 4262 4 4053 3 2991 8 2622 1 2039 2 1198 Name: IncomeGroup, dtype: int64 . plt.figure(figsize=(8,6),dpi=120) sns.barplot(data=train,x=&#39;IncomeGroup&#39;,y=&#39;MonthlyRevenue&#39;,estimator=np.mean) plt.xticks(rotation=90); . All Income groups contribute fairly to monthly revenue . plt.figure(figsize=(8,6),dpi=120) sns.barplot(data=train,x=&#39;CreditRating&#39;,y=&#39;MonthlyRevenue&#39;,estimator=np.mean) plt.xticks(rotation=90); . Low credit ratings contribute more to monthly revenue . category_col = list(categorical.columns) . category_col.remove(&#39;Churn&#39;) . train.isnull().sum() . CustomerID 0 Churn 0 MonthlyRevenue 156 MonthlyMinutes 156 TotalRecurringCharge 156 DirectorAssistedCalls 156 OverageMinutes 156 RoamingCalls 156 PercChangeMinutes 367 PercChangeRevenues 367 DroppedCalls 0 BlockedCalls 0 UnansweredCalls 0 CustomerCareCalls 0 ThreewayCalls 0 ReceivedCalls 0 OutboundCalls 0 InboundCalls 0 PeakCallsInOut 0 OffPeakCallsInOut 0 DroppedBlockedCalls 0 CallForwardingCalls 0 CallWaitingCalls 0 MonthsInService 0 UniqueSubs 0 ActiveSubs 0 ServiceArea 24 Handsets 1 HandsetModels 1 CurrentEquipmentDays 1 AgeHH1 909 AgeHH2 909 ChildrenInHH 0 HandsetRefurbished 0 HandsetWebCapable 0 TruckOwner 0 RVOwner 0 Homeownership 0 BuysViaMailOrder 0 RespondsToMailOffers 0 OptOutMailings 0 NonUSTravel 0 OwnsComputer 0 HasCreditCard 0 RetentionCalls 0 RetentionOffersAccepted 0 NewCellphoneUser 0 NotNewCellphoneUser 0 ReferralsMadeBySubscriber 0 IncomeGroup 0 OwnsMotorcycle 0 AdjustmentsToCreditRating 0 HandsetPrice 0 MadeCallToRetentionTeam 0 CreditRating 0 PrizmCode 0 Occupation 0 MaritalStatus 0 Subscription_months 0 dtype: int64 . train[(train[&#39;MonthlyRevenue&#39;].isnull()) &amp; (train[&#39;Churn&#39;]==&#39;Yes&#39;)] . CustomerID Churn MonthlyRevenue MonthlyMinutes TotalRecurringCharge DirectorAssistedCalls OverageMinutes RoamingCalls PercChangeMinutes PercChangeRevenues ... IncomeGroup OwnsMotorcycle AdjustmentsToCreditRating HandsetPrice MadeCallToRetentionTeam CreditRating PrizmCode Occupation MaritalStatus Subscription_months . 122 3000898 | Yes | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | 5 | No | 0 | 30 | No | 1-Highest | Other | Other | No | Less than six years | . 126 3000926 | Yes | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | 1 | No | 0 | 30 | No | 1-Highest | Town | Other | Unknown | Less than five years | . 925 3007326 | Yes | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | 4 | No | 0 | 60 | No | 3-Good | Other | Self | Yes | Less than five years | . 1454 3011438 | Yes | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | 5 | No | 1 | 30 | No | 1-Highest | Other | Other | Yes | Less than five years | . 2228 3017394 | Yes | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | 6 | No | 0 | Unknown | No | 5-Low | Suburban | Other | Yes | Less than four years | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 51010 3399690 | Yes | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | 4 | No | 0 | 60 | No | 3-Good | Other | Other | Yes | Less than four years | . 51011 3399698 | Yes | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | 6 | No | 0 | Unknown | No | 1-Highest | Other | Other | No | Less than four years | . 51026 3399830 | Yes | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | 9 | No | 0 | Unknown | No | 1-Highest | Suburban | Other | Unknown | Less than three years | . 51042 3399958 | Yes | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | 6 | No | 0 | 60 | No | 1-Highest | Suburban | Other | Yes | Less than three years | . 51044 3399978 | Yes | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | 7 | No | 1 | 80 | No | 5-Low | Other | Clerical | No | Less than three years | . 70 rows × 59 columns . train[train[&#39;PercChangeMinutes&#39;].isnull()] . CustomerID Churn MonthlyRevenue MonthlyMinutes TotalRecurringCharge DirectorAssistedCalls OverageMinutes RoamingCalls PercChangeMinutes PercChangeRevenues ... IncomeGroup OwnsMotorcycle AdjustmentsToCreditRating HandsetPrice MadeCallToRetentionTeam CreditRating PrizmCode Occupation MaritalStatus Subscription_months . 91 3000626 | No | 96.04 | 545.0 | 60.0 | 0.66 | 111.0 | 1.1 | NaN | NaN | ... | 0 | No | 0 | 60 | No | 1-Highest | Town | Other | Unknown | Less than six years | . 122 3000898 | Yes | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | 5 | No | 0 | 30 | No | 1-Highest | Other | Other | No | Less than six years | . 126 3000926 | Yes | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | 1 | No | 0 | 30 | No | 1-Highest | Town | Other | Unknown | Less than five years | . 461 3003534 | No | 52.85 | 662.0 | 57.0 | 0.33 | 0.0 | 5.0 | NaN | NaN | ... | 6 | No | 0 | 150 | Yes | 1-Highest | Suburban | Other | Unknown | Less than five years | . 641 3005090 | Yes | 5.00 | 0.0 | 0.0 | 0.00 | 0.0 | 0.0 | NaN | NaN | ... | 9 | No | 0 | Unknown | No | 3-Good | Suburban | Other | Yes | Less than five years | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 51038 3399910 | No | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | 7 | No | 0 | 30 | No | 1-Highest | Suburban | Other | Unknown | Less than three years | . 51042 3399958 | Yes | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | 6 | No | 0 | 60 | No | 1-Highest | Suburban | Other | Yes | Less than three years | . 51044 3399978 | Yes | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | 7 | No | 1 | 80 | No | 5-Low | Other | Clerical | No | Less than three years | . 51045 3399990 | No | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | 9 | No | 0 | 30 | No | 5-Low | Other | Other | No | Less than four years | . 51046 3399994 | No | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | 0 | No | 1 | 60 | Yes | 5-Low | Other | Other | Unknown | Less than three years | . 367 rows × 59 columns . Feature Engineering and Feature Selection . train.dropna(axis=0,inplace=True) test.dropna(axis=0,inplace=True) . X = train.drop(&#39;Churn&#39;,axis=1) y = train[&#39;Churn&#39;] . from sklearn.model_selection import train_test_split X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=101) . category_col.append(&#39;Subscription_months&#39;) . from feature_engine.encoding import OrdinalEncoder ordinal_encoder = OrdinalEncoder(variables=category_col) . y.replace([&#39;Yes&#39;,&#39;No&#39;],value=[1,0],inplace=True) . ordinal_encoder.fit(X_train,y_train) . OrdinalEncoder(variables=[&#39;ServiceArea&#39;, &#39;ChildrenInHH&#39;, &#39;HandsetRefurbished&#39;, &#39;HandsetWebCapable&#39;, &#39;TruckOwner&#39;, &#39;RVOwner&#39;, &#39;Homeownership&#39;, &#39;BuysViaMailOrder&#39;, &#39;RespondsToMailOffers&#39;, &#39;OptOutMailings&#39;, &#39;NonUSTravel&#39;, &#39;OwnsComputer&#39;, &#39;HasCreditCard&#39;, &#39;NewCellphoneUser&#39;, &#39;NotNewCellphoneUser&#39;, &#39;OwnsMotorcycle&#39;, &#39;HandsetPrice&#39;, &#39;MadeCallToRetentionTeam&#39;, &#39;CreditRating&#39;, &#39;PrizmCode&#39;, &#39;Occupation&#39;, &#39;MaritalStatus&#39;, &#39;Subscription_months&#39;, &#39;Subscription_months&#39;]) . X_train = ordinal_encoder.transform(X_train) X_test = ordinal_encoder.transform(X_test) . C: Users HP Downloads Anaconda_new lib site-packages feature_engine encoding base_encoder.py:193: UserWarning: During the encoding, NaN values were introduced in the feature(s) ServiceArea. warnings.warn( . from feature_engine.selection import DropConstantFeatures as dc from feature_engine.selection import DropDuplicateFeatures as ddup . sel = dc(tol=0.99) . sel.fit(X_train) . DropConstantFeatures(tol=0.99) . sel.features_to_drop_ . [&#39;CallForwardingCalls&#39;] . X_train = sel.transform(X_train) X_test = sel.transform(X_test) . sel = ddup() sel.fit(X_train) . DropDuplicateFeatures() . X_train = sel.transform(X_train) X_test = sel.transform(X_test) . from sklearn.ensemble import RandomForestClassifier from feature_engine.selection import SmartCorrelatedSelection as SCS . rf = RandomForestClassifier() sel = SCS(threshold=0.8,estimator=rf,cv=3) sel.fit(X_train,y_train) . SmartCorrelatedSelection(estimator=RandomForestClassifier()) . sel.features_to_drop_ . [&#39;ReceivedCalls&#39;, &#39;DroppedBlockedCalls&#39;, &#39;Handsets&#39;, &#39;BuysViaMailOrder&#39;, &#39;RetentionCalls&#39;] . sel.correlated_feature_sets_ . [{&#39;MonthlyMinutes&#39;, &#39;ReceivedCalls&#39;}, {&#39;BlockedCalls&#39;, &#39;DroppedBlockedCalls&#39;}, {&#39;HandsetModels&#39;, &#39;Handsets&#39;}, {&#39;BuysViaMailOrder&#39;, &#39;RespondsToMailOffers&#39;}, {&#39;MadeCallToRetentionTeam&#39;, &#39;RetentionCalls&#39;}] . X_train = sel.transform(X_train) X_test = sel.transform(X_test) . X_trainn = X_train . from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif . sel = SelectKBest(mutual_info_classif,k=20) . sel.fit(X_trainn,y_train) . SelectKBest(k=20, score_func=&lt;function mutual_info_classif at 0x0000022DCDBDAEE0&gt;) . . len(X_trainn) . 34826 . Model Building with 20 features . from sklearn.ensemble import RandomForestClassifier rfc = RandomForestClassifier() rfc.fit(X_train,y_train) . RandomForestClassifier() . rfc.feature_importances_ . array([0.04324872, 0.04047243, 0.04415805, 0.02816982, 0.01913889, 0.02440732, 0.01777274, 0.04602329, 0.03987401, 0.02777594, 0.02439108, 0.03352194, 0.01785917, 0.01024026, 0.03352286, 0.03185418, 0.02556385, 0.03466126, 0.03417809, 0.02947202, 0.00039164, 0.01597521, 0.03362669, 0.01107025, 0.00723789, 0.05509813, 0.00857872, 0.0069422 , 0.05325307, 0.02587271, 0.01862945, 0.00472835, 0.00464484, 0.00437891, 0.00421204, 0.0030057 , 0.00410281, 0.00412669, 0.00422763, 0.00126183, 0.0026187 , 0.00419523, 0.00395904, 0.00238723, 0.00093804, 0.00515096, 0.00436182, 0.00254065, 0.01891934, 0.00104458, 0.00206343, 0.01113174, 0.00231694, 0.01694146, 0.01282859, 0.0084616 , 0.00865103, 0.01381896]) . pred = rfc.predict(X_test) . Metrics for prediction . from sklearn.metrics import confusion_matrix, f1_score, classification_report . len(y_test) . 14890 . len(pred) . 14890 . confusion_matrix(y_test,pred) . array([[10335, 323], [ 3873, 359]], dtype=int64) . print(classification_report(y_test,pred)) . precision recall f1-score support 0 0.73 0.97 0.83 10658 1 0.53 0.08 0.15 4232 accuracy 0.72 14890 macro avg 0.63 0.53 0.49 14890 weighted avg 0.67 0.72 0.64 14890 . rfc_score = rfc.score(X_test,y_test) print(rfc_score) . 0.7182001343183344 . from sklearn.metrics import roc_curve pred_curve = rfc.predict_proba(X_test) curve = pred_curve[:,1] fpr, tpr, thresholds = roc_curve(y_test, curve) plt.subplot(331) plt.plot([0,1],[0,1],&#39;k--&#39;) plt.plot(fpr,tpr, label=&#39;ANN&#39;) plt.xlabel(&#39;fpr&#39;) plt.ylabel(&#39;tpr&#39;) plt.title(&#39;ROC Curve Random Forest&#39;) plt.grid(True) plt.subplots_adjust(top=2, bottom=0.08, left=0.10, right=1.4, hspace=0.45, wspace=0.45) plt.show() .",
            "url": "https://bilalkhan18.github.io/port/2022/09/15/Telecom_Churn.html",
            "relUrl": "/2022/09/15/Telecom_Churn.html",
            "date": " • Sep 15, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Sine Wave prediction using RNN and LSTM",
            "content": "Imports . import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns . Creating dataset . x = np.linspace(0,50,501) y = np.sin(x) . plt.plot(x,y) . [&lt;matplotlib.lines.Line2D at 0x18d2e083c40&gt;] . np.sin(20) . 0.9129452507276277 . np.sin(10) . -0.5440211108893698 . df = pd.DataFrame(data=y,index=x,columns=[&#39;Sine&#39;]) . df . Sine . 0.0 0.000000 | . 0.1 0.099833 | . 0.2 0.198669 | . 0.3 0.295520 | . 0.4 0.389418 | . ... ... | . 49.6 -0.617439 | . 49.7 -0.535823 | . 49.8 -0.448854 | . 49.9 -0.357400 | . 50.0 -0.262375 | . 501 rows × 1 columns . 0.1*len(df) . 50.1 . train = df.iloc[:451] test = df.iloc[451:] . train . Sine . 0.0 0.000000 | . 0.1 0.099833 | . 0.2 0.198669 | . 0.3 0.295520 | . 0.4 0.389418 | . ... ... | . 44.6 0.579164 | . 44.7 0.657656 | . 44.8 0.729577 | . 44.9 0.794208 | . 45.0 0.850904 | . 451 rows × 1 columns . test.head() . Sine . 45.1 0.899097 | . 45.2 0.938307 | . 45.3 0.968142 | . 45.4 0.988304 | . 45.5 0.998591 | . Scaling and creating sequences using TimeseriesGenerator . from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler() scaled_train = scaler.fit_transform(train) scaled_test = scaler.transform(test) . from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator . df.iloc[121] . Sine -0.449647 Name: 12.100000000000001, dtype: float64 . length = 120 n_features=1 . generator = TimeseriesGenerator(data=scaled_train,targets=scaled_train,length=length, batch_size=1) #validation_generator = TimeseriesGenerator(data=scaled_test,targets=scaled_test,length=length,batch_size=1) . generator[0] . (array([[[4.99999116e-01], [5.49916225e-01], [5.99334579e-01], [6.47760405e-01], [6.94709850e-01], [7.39713809e-01], [7.82322618e-01], [8.22110545e-01], [8.58680040e-01], [8.91665714e-01], [9.20737985e-01], [9.45606372e-01], [9.66022399e-01], [9.81782075e-01], [9.92727935e-01], [9.98750612e-01], [9.99789928e-01], [9.95835500e-01], [9.86926839e-01], [9.73152957e-01], [9.54651478e-01], [9.31607263e-01], [9.04250562e-01], [8.72854714e-01], [8.37733417e-01], [7.99237589e-01], [7.57751870e-01], [7.13690771e-01], [6.67494535e-01], [6.19624741e-01], [5.70559686e-01], [5.20789614e-01], [4.70811810e-01], [4.21125636e-01], [3.72227539e-01], [3.24606094e-01], [2.78737119e-01], [2.35078919e-01], [1.94067715e-01], [1.56113277e-01], [1.21594831e-01], [9.08572769e-02], [6.42077324e-02], [4.19124713e-02], [2.41942605e-02], [1.12301346e-02], [3.14962674e-03], [3.34747619e-05], [1.91281421e-03], [8.76886734e-03], [2.05331307e-02], [3.70880598e-02], [5.82682430e-02], [8.38620552e-02], [1.13613771e-01], [1.47226122e-01], [1.84363264e-01], [2.24654135e-01], [2.67696162e-01], [3.13059283e-01], [3.60290246e-01], [4.08917133e-01], [4.58454081e-01], [5.08406134e-01], [5.58274186e-01], [6.07559973e-01], [6.55771048e-01], [7.02425701e-01], [7.47057774e-01], [7.89221319e-01], [8.28495052e-01], [8.64486561e-01], [8.96836233e-01], [9.25220839e-01], [9.49356770e-01], [9.69002868e-01], [9.83962836e-01], [9.94087198e-01], [9.99274795e-01], [9.99473795e-01], [9.94682209e-01], [9.84947913e-01], [9.70368169e-01], [9.51088653e-01], [9.27301999e-01], [8.99245876e-01], [8.67200612e-01], [8.31486391e-01], [7.92460059e-01], [7.50511555e-01], [7.06060012e-01], [6.59549578e-01], [6.11444967e-01], [5.62226827e-01], [5.12386928e-01], [4.62423254e-01], [4.12835026e-01], [3.64117712e-01], [3.16758081e-01], [2.71229333e-01], [2.27986377e-01], [1.87461283e-01], [1.50058964e-01], [1.16153131e-01], [8.60825596e-02], [6.01477060e-02], [3.86077023e-02], [2.16777691e-02], [9.52706470e-03], [2.27699490e-03], [0.00000000e+00], [2.71883099e-03], [1.04063222e-02], [2.29856628e-02], [4.03311641e-02], [6.22695157e-02], [8.85815167e-02], [1.19004266e-01], [1.53233791e-01], [1.90928079e-01]]]), array([[0.2317105]])) . generator[1] . (array([[[5.49916225e-01], [5.99334579e-01], [6.47760405e-01], [6.94709850e-01], [7.39713809e-01], [7.82322618e-01], [8.22110545e-01], [8.58680040e-01], [8.91665714e-01], [9.20737985e-01], [9.45606372e-01], [9.66022399e-01], [9.81782075e-01], [9.92727935e-01], [9.98750612e-01], [9.99789928e-01], [9.95835500e-01], [9.86926839e-01], [9.73152957e-01], [9.54651478e-01], [9.31607263e-01], [9.04250562e-01], [8.72854714e-01], [8.37733417e-01], [7.99237589e-01], [7.57751870e-01], [7.13690771e-01], [6.67494535e-01], [6.19624741e-01], [5.70559686e-01], [5.20789614e-01], [4.70811810e-01], [4.21125636e-01], [3.72227539e-01], [3.24606094e-01], [2.78737119e-01], [2.35078919e-01], [1.94067715e-01], [1.56113277e-01], [1.21594831e-01], [9.08572769e-02], [6.42077324e-02], [4.19124713e-02], [2.41942605e-02], [1.12301346e-02], [3.14962674e-03], [3.34747619e-05], [1.91281421e-03], [8.76886734e-03], [2.05331307e-02], [3.70880598e-02], [5.82682430e-02], [8.38620552e-02], [1.13613771e-01], [1.47226122e-01], [1.84363264e-01], [2.24654135e-01], [2.67696162e-01], [3.13059283e-01], [3.60290246e-01], [4.08917133e-01], [4.58454081e-01], [5.08406134e-01], [5.58274186e-01], [6.07559973e-01], [6.55771048e-01], [7.02425701e-01], [7.47057774e-01], [7.89221319e-01], [8.28495052e-01], [8.64486561e-01], [8.96836233e-01], [9.25220839e-01], [9.49356770e-01], [9.69002868e-01], [9.83962836e-01], [9.94087198e-01], [9.99274795e-01], [9.99473795e-01], [9.94682209e-01], [9.84947913e-01], [9.70368169e-01], [9.51088653e-01], [9.27301999e-01], [8.99245876e-01], [8.67200612e-01], [8.31486391e-01], [7.92460059e-01], [7.50511555e-01], [7.06060012e-01], [6.59549578e-01], [6.11444967e-01], [5.62226827e-01], [5.12386928e-01], [4.62423254e-01], [4.12835026e-01], [3.64117712e-01], [3.16758081e-01], [2.71229333e-01], [2.27986377e-01], [1.87461283e-01], [1.50058964e-01], [1.16153131e-01], [8.60825596e-02], [6.01477060e-02], [3.86077023e-02], [2.16777691e-02], [9.52706470e-03], [2.27699490e-03], [0.00000000e+00], [2.71883099e-03], [1.04063222e-02], [2.29856628e-02], [4.03311641e-02], [6.22695157e-02], [8.85815167e-02], [1.19004266e-01], [1.53233791e-01], [1.90928079e-01], [2.31710504e-01]]]), array([[0.27517358]])) . . Creating RNN Model . from tensorflow.keras.models import Sequential from tensorflow.keras.layers import SimpleRNN,Dense from tensorflow.keras.callbacks import EarlyStopping . early_stop = EarlyStopping(patience=3) . model = Sequential() model.add(SimpleRNN(120,input_shape=(length,n_features))) model.add(Dense(1)) model.compile(optimizer=&#39;adam&#39;,loss=&#39;mse&#39;) . model.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= simple_rnn (SimpleRNN) (None, 120) 14640 dense (Dense) (None, 1) 121 ================================================================= Total params: 14,761 Trainable params: 14,761 Non-trainable params: 0 _________________________________________________________________ . model.fit(generator, epochs=8, callbacks=[early_stop]) . Epoch 1/8 331/331 [==============================] - ETA: 0s - loss: 0.0023WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss 331/331 [==============================] - 18s 34ms/step - loss: 0.0023 Epoch 2/8 330/331 [============================&gt;.] - ETA: 0s - loss: 0.0034WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss 331/331 [==============================] - 9s 28ms/step - loss: 0.0034 Epoch 3/8 330/331 [============================&gt;.] - ETA: 0s - loss: 3.2949e-04WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss 331/331 [==============================] - 10s 29ms/step - loss: 3.2849e-04 Epoch 4/8 330/331 [============================&gt;.] - ETA: 0s - loss: 7.3571e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss 331/331 [==============================] - 10s 29ms/step - loss: 7.3349e-05 Epoch 5/8 331/331 [==============================] - ETA: 0s - loss: 7.5699e-06WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss 331/331 [==============================] - 10s 31ms/step - loss: 7.5699e-06 Epoch 6/8 331/331 [==============================] - ETA: 0s - loss: 2.0096e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss 331/331 [==============================] - 9s 28ms/step - loss: 2.0096e-05 Epoch 7/8 331/331 [==============================] - ETA: 0s - loss: 7.2813e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss 331/331 [==============================] - 9s 28ms/step - loss: 7.2813e-05 Epoch 8/8 331/331 [==============================] - ETA: 0s - loss: 3.8718e-06WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss 331/331 [==============================] - 10s 30ms/step - loss: 3.8718e-06 . &lt;keras.callbacks.History at 0x18d36df3100&gt; . . first_batch = scaled_train[-length:] . first_batch.shape . (120, 1) . first_batch = first_batch.reshape((1,length,n_features)) . Predictions using RNN . model.predict(first_batch) . array([[0.9499798]], dtype=float32) . scaled_test[0] . array([0.94955134]) . test_pred = [] first_batch = scaled_train[-length:] current_batch = first_batch.reshape((1,length,n_features)) for i in range(len(test)): current_pred = model.predict(current_batch)[0] test_pred.append(current_pred) current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1) . test_pred . [array([0.9499798], dtype=float32), array([0.9694884], dtype=float32), array([0.9844099], dtype=float32), array([0.99429387], dtype=float32), array([0.99932206], dtype=float32), array([0.9990793], dtype=float32), array([0.9939744], dtype=float32), array([0.9838967], dtype=float32), array([0.96927893], dtype=float32), array([0.95011795], dtype=float32), array([0.9268703], dtype=float32), array([0.899651], dtype=float32), array([0.86894196], dtype=float32), array([0.83496636], dtype=float32), array([0.7980918], dtype=float32), array([0.75866556], dtype=float32), array([0.71702135], dtype=float32), array([0.6735083], dtype=float32), array([0.62848186], dtype=float32), array([0.5823541], dtype=float32), array([0.5355599], dtype=float32), array([0.48840505], dtype=float32), array([0.44141504], dtype=float32), array([0.39500633], dtype=float32), array([0.34952673], dtype=float32), array([0.30553782], dtype=float32), array([0.26341128], dtype=float32), array([0.22366914], dtype=float32), array([0.18665712], dtype=float32), array([0.1528996], dtype=float32), array([0.12261443], dtype=float32), array([0.09634649], dtype=float32), array([0.07420367], dtype=float32), array([0.05664599], dtype=float32), array([0.0436875], dtype=float32), array([0.03566788], dtype=float32), array([0.03248379], dtype=float32), array([0.03440125], dtype=float32), array([0.04118868], dtype=float32), array([0.05296577], dtype=float32), array([0.06939272], dtype=float32), array([0.09044757], dtype=float32), array([0.11575846], dtype=float32), array([0.14512841], dtype=float32), array([0.17810209], dtype=float32), array([0.21435443], dtype=float32), array([0.2534281], dtype=float32), array([0.29491204], dtype=float32), array([0.3383421], dtype=float32), array([0.38326505], dtype=float32)] . predictions = scaler.inverse_transform(test_pred) . test[&#39;Prediction&#39;] = predictions . C: Users HP AppData Local Temp/ipykernel_10532/2447167120.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy test[&#39;Prediction&#39;] = predictions . test . Sine Prediction . 45.1 0.899097 | 0.899954 | . 45.2 0.938307 | 0.938971 | . 45.3 0.968142 | 0.968814 | . 45.4 0.988304 | 0.988582 | . 45.5 0.998591 | 0.998638 | . 45.6 0.998900 | 0.998152 | . 45.7 0.989229 | 0.987943 | . 45.8 0.969673 | 0.967787 | . 45.9 0.940429 | 0.938552 | . 46.0 0.901788 | 0.900230 | . 46.1 0.854137 | 0.853735 | . 46.2 0.797952 | 0.799297 | . 46.3 0.733794 | 0.737880 | . 46.4 0.662304 | 0.669929 | . 46.5 0.584197 | 0.596181 | . 46.6 0.500252 | 0.517329 | . 46.7 0.411309 | 0.434041 | . 46.8 0.318257 | 0.347016 | . 46.9 0.222024 | 0.256963 | . 47.0 0.123573 | 0.164709 | . 47.1 0.023888 | 0.071121 | . 47.2 -0.076037 | -0.023188 | . 47.3 -0.175201 | -0.117167 | . 47.4 -0.272615 | -0.209984 | . 47.5 -0.367305 | -0.300942 | . 47.6 -0.458325 | -0.388919 | . 47.7 -0.544766 | -0.473172 | . 47.8 -0.625764 | -0.552656 | . 47.9 -0.700509 | -0.626679 | . 48.0 -0.768255 | -0.694193 | . 48.1 -0.828324 | -0.754763 | . 48.2 -0.880118 | -0.807299 | . 48.3 -0.923117 | -0.851584 | . 48.4 -0.956893 | -0.886699 | . 48.5 -0.981108 | -0.912616 | . 48.6 -0.995521 | -0.928655 | . 48.7 -0.999986 | -0.935023 | . 48.8 -0.994460 | -0.931188 | . 48.9 -0.978997 | -0.917614 | . 49.0 -0.953753 | -0.894060 | . 49.1 -0.918979 | -0.861206 | . 49.2 -0.875023 | -0.819097 | . 49.3 -0.822324 | -0.768475 | . 49.4 -0.761408 | -0.709736 | . 49.5 -0.692885 | -0.643789 | . 49.6 -0.617439 | -0.571285 | . 49.7 -0.535823 | -0.493138 | . 49.8 -0.448854 | -0.410171 | . 49.9 -0.357400 | -0.323311 | . 50.0 -0.262375 | -0.233466 | . test.plot() . &lt;AxesSubplot:&gt; . Using LSTM . from tensorflow.keras.layers import LSTM . model = Sequential() model.add(LSTM(120,input_shape=(length,n_features))) model.add(Dense(1)) model.compile(optimizer=&#39;adam&#39;,loss=&#39;mse&#39;) . model.fit(generator,epochs = 8) . Epoch 1/8 331/331 [==============================] - 22s 59ms/step - loss: 0.0097 Epoch 2/8 331/331 [==============================] - 17s 52ms/step - loss: 5.3544e-04 Epoch 3/8 331/331 [==============================] - 17s 52ms/step - loss: 1.9293e-04 Epoch 4/8 331/331 [==============================] - 17s 52ms/step - loss: 2.2856e-04 Epoch 5/8 331/331 [==============================] - 17s 53ms/step - loss: 4.8776e-05 Epoch 6/8 331/331 [==============================] - 18s 53ms/step - loss: 6.8765e-05 Epoch 7/8 331/331 [==============================] - 18s 54ms/step - loss: 1.1893e-04 Epoch 8/8 331/331 [==============================] - 19s 58ms/step - loss: 9.0045e-04 . &lt;keras.callbacks.History at 0x18d394e4400&gt; . Predictions using LSTM . test_pred = [] first_batch = scaled_train[-length:] current_batch = first_batch.reshape((1,length,n_features)) for i in range(len(test)): current_pred = model.predict(current_batch)[0] test_pred.append(current_pred) current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1) . predictions = scaler.inverse_transform(test_pred) . test[&#39;LSTM Pred&#39;] = predictions . C: Users HP AppData Local Temp/ipykernel_10532/3796240138.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy test[&#39;LSTM Pred&#39;] = predictions . Comparing RNN and LSTM Predictions with the original label . test . Sine Prediction LSTM Pred . 45.1 0.899097 | 0.899954 | 0.930114 | . 45.2 0.938307 | 0.938971 | 0.970383 | . 45.3 0.968142 | 0.968814 | 1.003538 | . 45.4 0.988304 | 0.988582 | 1.029027 | . 45.5 0.998591 | 0.998638 | 1.046510 | . 45.6 0.998900 | 0.998152 | 1.055800 | . 45.7 0.989229 | 0.987943 | 1.056830 | . 45.8 0.969673 | 0.967787 | 1.049636 | . 45.9 0.940429 | 0.938552 | 1.034343 | . 46.0 0.901788 | 0.900230 | 1.011147 | . 46.1 0.854137 | 0.853735 | 0.980314 | . 46.2 0.797952 | 0.799297 | 0.942164 | . 46.3 0.733794 | 0.737880 | 0.897072 | . 46.4 0.662304 | 0.669929 | 0.845454 | . 46.5 0.584197 | 0.596181 | 0.787771 | . 46.6 0.500252 | 0.517329 | 0.724519 | . 46.7 0.411309 | 0.434041 | 0.656232 | . 46.8 0.318257 | 0.347016 | 0.583476 | . 46.9 0.222024 | 0.256963 | 0.506855 | . 47.0 0.123573 | 0.164709 | 0.427003 | . 47.1 0.023888 | 0.071121 | 0.344587 | . 47.2 -0.076037 | -0.023188 | 0.260305 | . 47.3 -0.175201 | -0.117167 | 0.174882 | . 47.4 -0.272615 | -0.209984 | 0.089069 | . 47.5 -0.367305 | -0.300942 | 0.003633 | . 47.6 -0.458325 | -0.388919 | -0.080646 | . 47.7 -0.544766 | -0.473172 | -0.162984 | . 47.8 -0.625764 | -0.552656 | -0.242605 | . 47.9 -0.700509 | -0.626679 | -0.318748 | . 48.0 -0.768255 | -0.694193 | -0.390677 | . 48.1 -0.828324 | -0.754763 | -0.457694 | . 48.2 -0.880118 | -0.807299 | -0.519144 | . 48.3 -0.923117 | -0.851584 | -0.574425 | . 48.4 -0.956893 | -0.886699 | -0.622995 | . 48.5 -0.981108 | -0.912616 | -0.664374 | . 48.6 -0.995521 | -0.928655 | -0.698152 | . 48.7 -0.999986 | -0.935023 | -0.723986 | . 48.8 -0.994460 | -0.931188 | -0.741607 | . 48.9 -0.978997 | -0.917614 | -0.750813 | . 49.0 -0.953753 | -0.894060 | -0.751478 | . 49.1 -0.918979 | -0.861206 | -0.743543 | . 49.2 -0.875023 | -0.819097 | -0.727022 | . 49.3 -0.822324 | -0.768475 | -0.701997 | . 49.4 -0.761408 | -0.709736 | -0.668621 | . 49.5 -0.692885 | -0.643789 | -0.627115 | . 49.6 -0.617439 | -0.571285 | -0.577771 | . 49.7 -0.535823 | -0.493138 | -0.520949 | . 49.8 -0.448854 | -0.410171 | -0.457082 | . 49.9 -0.357400 | -0.323311 | -0.386672 | . 50.0 -0.262375 | -0.233466 | -0.310290 | . test.plot() . &lt;AxesSubplot:&gt; . Since the sequence of the data is not long, RNN has performed better. However for Data having lengthy Sequence, use of LSTM would be an optimal choice. . Forecasting . full_scaler = MinMaxScaler() scaled_data = full_scaler.fit_transform(df) . length = 120 n_features = 1 . generator = TimeseriesGenerator(data=scaled_data,targets=scaled_data,length=length,batch_size=1) . model = Sequential() model.add(SimpleRNN(120,input_shape=(length,n_features))) model.add(Dense(1)) model.compile(optimizer=&#39;adam&#39;, loss=&#39;mse&#39;) . model.fit(generator,epochs=8) . Epoch 1/8 381/381 [==============================] - 12s 28ms/step - loss: 0.0061 Epoch 2/8 381/381 [==============================] - 13s 33ms/step - loss: 6.7171e-05 Epoch 3/8 381/381 [==============================] - 11s 28ms/step - loss: 7.7792e-04 Epoch 4/8 381/381 [==============================] - 10s 27ms/step - loss: 2.3956e-05 Epoch 5/8 381/381 [==============================] - 10s 27ms/step - loss: 2.7966e-05 Epoch 6/8 381/381 [==============================] - 11s 28ms/step - loss: 2.9440e-04 Epoch 7/8 381/381 [==============================] - 11s 28ms/step - loss: 6.4102e-06 Epoch 8/8 381/381 [==============================] - 11s 28ms/step - loss: 1.4723e-05 . &lt;keras.callbacks.History at 0x18d382f98e0&gt; . forecast = [] first_batch = scaled_data[-length:] current_batch = first_batch.reshape((1,length,n_features)) for i in range(len(test)): current_pred = model.predict(current_batch)[0] forecast.append(current_pred) current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1) . forecast = full_scaler.inverse_transform(forecast) . len(forecast) . 50 . forecast_index = np.arange(50.1,55.1,0.1) . Forecasting using RNN . plt.plot(df.index,df[&#39;Sine&#39;]) plt.plot(forecast_index,forecast) . [&lt;matplotlib.lines.Line2D at 0x18d3ce1e490&gt;] . Forecasting using LSTM . model = Sequential() model.add(LSTM(120,input_shape=(length,n_features))) model.add(Dense(1)) model.compile(optimizer=&#39;adam&#39;, loss=&#39;mse&#39;) . model.fit(generator,epochs=8) . Epoch 1/8 381/381 [==============================] - 24s 57ms/step - loss: 0.0065 Epoch 2/8 381/381 [==============================] - 20s 53ms/step - loss: 8.6971e-04 Epoch 3/8 381/381 [==============================] - 20s 52ms/step - loss: 1.5827e-04 Epoch 4/8 381/381 [==============================] - 20s 52ms/step - loss: 4.5681e-05 Epoch 5/8 381/381 [==============================] - 20s 51ms/step - loss: 5.7769e-05 Epoch 6/8 381/381 [==============================] - 20s 52ms/step - loss: 1.3684e-04 Epoch 7/8 381/381 [==============================] - 21s 56ms/step - loss: 8.3852e-04 Epoch 8/8 381/381 [==============================] - 21s 54ms/step - loss: 8.2290e-05 . &lt;keras.callbacks.History at 0x18d3a7617f0&gt; . forecast = [] first_batch = scaled_data[-length:] current_batch = first_batch.reshape((1,length,n_features)) for i in range(len(test)): current_pred = model.predict(current_batch)[0] forecast.append(current_pred) current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1) . plt.plot(df.index,df[&#39;Sine&#39;]) plt.plot(forecast_index,forecast) . [&lt;matplotlib.lines.Line2D at 0x18d4014adc0&gt;] . This shows that for data with large sequence LSTM is an optimal choice, however we can improve the model performance by increasing the number of epochs which currently has been trained at 8 epochs. .",
            "url": "https://bilalkhan18.github.io/port/2022/09/15/SineWave.html",
            "relUrl": "/2022/09/15/SineWave.html",
            "date": " • Sep 15, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Credit Card Fraud Detection",
            "content": "import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns . df = pd.read_csv(&#39;C: Users HP Downloads card_transdata.csv&#39;) . df.head() . distance_from_home distance_from_last_transaction ratio_to_median_purchase_price repeat_retailer used_chip used_pin_number online_order fraud . 0 57.877857 | 0.311140 | 1.945940 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | . 1 10.829943 | 0.175592 | 1.294219 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2 5.091079 | 0.805153 | 0.427715 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | . 3 2.247564 | 5.600044 | 0.362663 | 1.0 | 1.0 | 0.0 | 1.0 | 0.0 | . 4 44.190936 | 0.566486 | 2.222767 | 1.0 | 1.0 | 0.0 | 1.0 | 0.0 | . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1000000 entries, 0 to 999999 Data columns (total 8 columns): # Column Non-Null Count Dtype -- -- 0 distance_from_home 1000000 non-null float64 1 distance_from_last_transaction 1000000 non-null float64 2 ratio_to_median_purchase_price 1000000 non-null float64 3 repeat_retailer 1000000 non-null float64 4 used_chip 1000000 non-null float64 5 used_pin_number 1000000 non-null float64 6 online_order 1000000 non-null float64 7 fraud 1000000 non-null float64 dtypes: float64(8) memory usage: 61.0 MB . df.describe().transpose() . count mean std min 25% 50% 75% max . distance_from_home 1000000.0 | 26.628792 | 65.390784 | 0.004874 | 3.878008 | 9.967760 | 25.743985 | 10632.723672 | . distance_from_last_transaction 1000000.0 | 5.036519 | 25.843093 | 0.000118 | 0.296671 | 0.998650 | 3.355748 | 11851.104565 | . ratio_to_median_purchase_price 1000000.0 | 1.824182 | 2.799589 | 0.004399 | 0.475673 | 0.997717 | 2.096370 | 267.802942 | . repeat_retailer 1000000.0 | 0.881536 | 0.323157 | 0.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | . used_chip 1000000.0 | 0.350399 | 0.477095 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | 1.000000 | . used_pin_number 1000000.0 | 0.100608 | 0.300809 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | . online_order 1000000.0 | 0.650552 | 0.476796 | 0.000000 | 0.000000 | 1.000000 | 1.000000 | 1.000000 | . fraud 1000000.0 | 0.087403 | 0.282425 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | . df.corr()[&#39;fraud&#39;].sort_values() . used_pin_number -0.100293 used_chip -0.060975 repeat_retailer -0.001357 distance_from_last_transaction 0.091917 distance_from_home 0.187571 online_order 0.191973 ratio_to_median_purchase_price 0.462305 fraud 1.000000 Name: fraud, dtype: float64 . cor = df.corr() . plt.figure(figsize=(8,6),dpi=120) sns.heatmap(cor) . &lt;AxesSubplot:&gt; . plt.figure(figsize=(8,6),dpi=120) sns.scatterplot(data=df,y=&#39;ratio_to_median_purchase_price&#39;,x=&#39;distance_from_home&#39;,hue=&#39;fraud&#39;,alpha=0.7) . &lt;AxesSubplot:xlabel=&#39;distance_from_home&#39;, ylabel=&#39;ratio_to_median_purchase_price&#39;&gt; . 3 data points can be considered as ouliers, and hence are removed below . df[df[&#39;distance_from_home&#39;]&gt;5000] . distance_from_home distance_from_last_transaction ratio_to_median_purchase_price repeat_retailer used_chip used_pin_number online_order fraud . 266995 5797.972589 | 0.918924 | 2.359630 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 326661 10632.723672 | 1.151871 | 3.310613 | 1.0 | 0.0 | 0.0 | 1.0 | 1.0 | . 847723 8777.136420 | 0.207396 | 3.441051 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | . df.drop([266995,326661,847723],axis=0,inplace=True) . plt.figure(figsize=(8,6),dpi=120) sns.kdeplot(data=df,x=&#39;distance_from_home&#39;) . &lt;AxesSubplot:xlabel=&#39;distance_from_home&#39;, ylabel=&#39;Density&#39;&gt; . plt.figure(figsize=(8,6),dpi=120) sns.countplot(data=df,x=&#39;fraud&#39;) . &lt;AxesSubplot:xlabel=&#39;fraud&#39;, ylabel=&#39;count&#39;&gt; . plt.figure(figsize=(8,6),dpi=120) sns.countplot(data=df,x=&#39;repeat_retailer&#39;,hue=&#39;fraud&#39;) . &lt;AxesSubplot:xlabel=&#39;repeat_retailer&#39;, ylabel=&#39;count&#39;&gt; . plt.figure(figsize=(8,6),dpi=120) sns.countplot(data=df,x=&#39;online_order&#39;,hue=&#39;fraud&#39;) . &lt;AxesSubplot:xlabel=&#39;online_order&#39;, ylabel=&#39;count&#39;&gt; . plt.figure(figsize=(8,6),dpi=120) sns.scatterplot(data=df,y=&#39;distance_from_last_transaction&#39;,x=&#39;distance_from_home&#39;,hue=&#39;fraud&#39;,alpha=0.7) . &lt;AxesSubplot:xlabel=&#39;distance_from_home&#39;, ylabel=&#39;distance_from_last_transaction&#39;&gt; . There seems to be no linear relationship among features . df.isnull().sum() . distance_from_home 0 distance_from_last_transaction 0 ratio_to_median_purchase_price 0 repeat_retailer 0 used_chip 0 used_pin_number 0 online_order 0 fraud 0 dtype: int64 . No missing values in the dataset . from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split . X = df.drop(&#39;fraud&#39;,axis=1) y = df[&#39;fraud&#39;] . X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=101) . scaler = StandardScaler() scaler.fit(X_train) X_train = scaler.transform(X_train) X_test = scaler.transform(X_test) . from sklearn.ensemble import RandomForestClassifier from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC from sklearn.neighbors import KNeighborsClassifier . from sklearn.metrics import classification_report, confusion_matrix, roc_curve, f1_score from sklearn.metrics import plot_roc_curve . rf = RandomForestClassifier() lg = LogisticRegression() sv = SVC() knn = KNeighborsClassifier() . models = [rf,lg,sv,knn] . rf.fit(X_train,y_train) . RandomForestClassifier() . In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier() . pred = rf.predict(X_test) print(classification_report(y_test,pred)) plot_roc_curve(rf,X_test,y_test) . precision recall f1-score support 0.0 1.00 1.00 1.00 273760 1.0 1.00 1.00 1.00 26240 accuracy 1.00 300000 macro avg 1.00 1.00 1.00 300000 weighted avg 1.00 1.00 1.00 300000 . C: Users HP Downloads Anaconda_new lib site-packages sklearn utils deprecation.py:87: FutureWarning: Function plot_roc_curve is deprecated; Function :func:`plot_roc_curve` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: :meth:`sklearn.metrics.RocCurveDisplay.from_predictions` or :meth:`sklearn.metrics.RocCurveDisplay.from_estimator`. warnings.warn(msg, category=FutureWarning) . &lt;sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x2353b42c100&gt; . Random Forest performs well with 100% precision and recall. Linear model was not used as there was no linear relationship between features and label. .",
            "url": "https://bilalkhan18.github.io/port/2022/09/14/CreditcardFraud.html",
            "relUrl": "/2022/09/14/CreditcardFraud.html",
            "date": " • Sep 14, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Medical Insurance Prediction",
            "content": "import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns . df = pd.read_csv(&#39;C: Users HP Downloads archive (97) insurance.csv&#39;) . . age: age of primary beneficiary . sex: insurance contractor gender, female, male . bmi: Body mass index, providing an understanding of body, weights that are relatively high or low relative to height, objective index of body weight (kg / m ^ 2) using the ratio of height to weight, ideally 18.5 to 24.9 . children: Number of children covered by health insurance / Number of dependents . smoker: Smoking . region: the beneficiary&#39;s residential area in the US, northeast, southeast, southwest, northwest. . charges: Individual medical costs billed by health insurance . df.head() . age sex bmi children smoker region charges . 0 19 | female | 27.900 | 0 | yes | southwest | 16884.92400 | . 1 18 | male | 33.770 | 1 | no | southeast | 1725.55230 | . 2 28 | male | 33.000 | 3 | no | southeast | 4449.46200 | . 3 33 | male | 22.705 | 0 | no | northwest | 21984.47061 | . 4 32 | male | 28.880 | 0 | no | northwest | 3866.85520 | . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1338 entries, 0 to 1337 Data columns (total 7 columns): # Column Non-Null Count Dtype -- -- 0 age 1338 non-null int64 1 sex 1338 non-null object 2 bmi 1338 non-null float64 3 children 1338 non-null int64 4 smoker 1338 non-null object 5 region 1338 non-null object 6 charges 1338 non-null float64 dtypes: float64(2), int64(2), object(3) memory usage: 73.3+ KB . df.describe().transpose() . count mean std min 25% 50% 75% max . age 1338.0 | 39.207025 | 14.049960 | 18.0000 | 27.00000 | 39.000 | 51.000000 | 64.00000 | . bmi 1338.0 | 30.663397 | 6.098187 | 15.9600 | 26.29625 | 30.400 | 34.693750 | 53.13000 | . children 1338.0 | 1.094918 | 1.205493 | 0.0000 | 0.00000 | 1.000 | 2.000000 | 5.00000 | . charges 1338.0 | 13270.422265 | 12110.011237 | 1121.8739 | 4740.28715 | 9382.033 | 16639.912515 | 63770.42801 | . numeric_data = df[[&#39;age&#39;,&#39;bmi&#39;,&#39;children&#39;,&#39;charges&#39;]] . EDA . sns.displot(data=numeric_data, x=&#39;bmi&#39;,kind=&#39;kde&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x1ea7c43c8e0&gt; . sns.displot(data=numeric_data, x=&#39;age&#39;,kind=&#39;kde&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x1ea7c4124c0&gt; . sns.displot(data=numeric_data, x=&#39;charges&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x1ea7d878af0&gt; . numeric_data[&#39;children&#39;].value_counts() . 0 574 1 324 2 240 3 157 4 25 5 18 Name: children, dtype: int64 . plt.figure(figsize=(10,6),dpi=120) sns.countplot(data=df, x=&#39;children&#39;) . &lt;AxesSubplot:xlabel=&#39;children&#39;, ylabel=&#39;count&#39;&gt; . We shall consider these as categories and create a label of &#39;3 or more&#39; which includes rows having 3,4 and 5 children . correlation = numeric_data.corr() plt.figure(figsize=(8,6),dpi=120) sns.heatmap(data=correlation) . &lt;AxesSubplot:&gt; . numeric_data.corr()[&#39;charges&#39;].sort_values() . children 0.067998 bmi 0.198341 age 0.299008 charges 1.000000 Name: charges, dtype: float64 . plt.figure(figsize=(8,6),dpi=120) sns.scatterplot(data=df, x=&#39;age&#39;,y=&#39;charges&#39;) . &lt;AxesSubplot:xlabel=&#39;age&#39;, ylabel=&#39;charges&#39;&gt; . plt.figure(figsize=(8,6),dpi=120) sns.scatterplot(data=df, x=&#39;age&#39;,y=&#39;charges&#39;,hue=&#39;smoker&#39;) . &lt;AxesSubplot:xlabel=&#39;age&#39;, ylabel=&#39;charges&#39;&gt; . Insight 01: People who smoke have higher charges as comapared to non-smokers of the same age group . df.head() . age sex bmi children smoker region charges . 0 19 | female | 27.900 | 0 | yes | southwest | 16884.92400 | . 1 18 | male | 33.770 | 1 | no | southeast | 1725.55230 | . 2 28 | male | 33.000 | 3 | no | southeast | 4449.46200 | . 3 33 | male | 22.705 | 0 | no | northwest | 21984.47061 | . 4 32 | male | 28.880 | 0 | no | northwest | 3866.85520 | . plt.figure(figsize=(8,6),dpi=120) sns.scatterplot(data=df, x=&#39;age&#39;,y=&#39;charges&#39;,hue=&#39;sex&#39;) . &lt;AxesSubplot:xlabel=&#39;age&#39;, ylabel=&#39;charges&#39;&gt; . Insight 02: With increase of age, charges also increase regardless of gender. . plt.figure(figsize=(8,6),dpi=120) sns.scatterplot(data=df, x=&#39;children&#39;,y=&#39;charges&#39;) . &lt;AxesSubplot:xlabel=&#39;children&#39;, ylabel=&#39;charges&#39;&gt; . Insight 03: Number of children alone do not impact charges . plt.figure(figsize=(8,6),dpi=120) sns.scatterplot(data=df, x=&#39;bmi&#39;,y=&#39;charges&#39;,hue=&#39;sex&#39;,alpha=0.8) . &lt;AxesSubplot:xlabel=&#39;bmi&#39;, ylabel=&#39;charges&#39;&gt; . Insight 04: There is a slight linear increase in charges with increase in bmi . plt.figure(figsize=(8,6),dpi=120) sns.scatterplot(data=df,x=&#39;bmi&#39;,y=&#39;charges&#39;,hue=&#39;smoker&#39;) . &lt;AxesSubplot:xlabel=&#39;bmi&#39;, ylabel=&#39;charges&#39;&gt; . Insight 05: A person who smokes will have more charges as comapred to non-smoker of the same bmi . df.head() . age sex bmi children smoker region charges . 0 19 | female | 27.900 | 0 | yes | southwest | 16884.92400 | . 1 18 | male | 33.770 | 1 | no | southeast | 1725.55230 | . 2 28 | male | 33.000 | 3 | no | southeast | 4449.46200 | . 3 33 | male | 22.705 | 0 | no | northwest | 21984.47061 | . 4 32 | male | 28.880 | 0 | no | northwest | 3866.85520 | . def personality_check(bmi): if bmi &lt;= 18.5: return &#39;underweight&#39; elif (24.9 &gt; bmi &gt; 18.5): return &#39;normal&#39; elif (29.9&gt;bmi&gt;25): return &#39;overweight&#39; elif(bmi&gt;= 30): return &#39;obese&#39; df[&#39;personality&#39;] = df[&#39;bmi&#39;].apply(personality_check) . df.head() . age sex bmi children smoker region charges personality . 0 19 | female | 27.900 | 0 | yes | southwest | 16884.92400 | overweight | . 1 18 | male | 33.770 | 1 | no | southeast | 1725.55230 | obese | . 2 28 | male | 33.000 | 3 | no | southeast | 4449.46200 | obese | . 3 33 | male | 22.705 | 0 | no | northwest | 21984.47061 | normal | . 4 32 | male | 28.880 | 0 | no | northwest | 3866.85520 | overweight | . def smoke_personality(personality,smoker): if personality == &#39;obese&#39; and smoker == &#39;yes&#39;: return 1 else: return 0 df[&#39;obese_smoker&#39;] = df[[&#39;personality&#39;,&#39;smoker&#39;]].apply(lambda df: smoke_personality(df[&#39;personality&#39;],df[&#39;smoker&#39;]),axis=1) . df[df[&#39;obese_smoker&#39;]==1] . age sex bmi children smoker region charges personality obese_smoker . 14 27 | male | 42.130 | 0 | yes | southeast | 39611.75770 | obese | 1 | . 19 30 | male | 35.300 | 0 | yes | southwest | 36837.46700 | obese | 1 | . 23 34 | female | 31.920 | 1 | yes | northeast | 37701.87680 | obese | 1 | . 29 31 | male | 36.300 | 2 | yes | southwest | 38711.00000 | obese | 1 | . 30 22 | male | 35.600 | 0 | yes | southwest | 35585.57600 | obese | 1 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 1300 45 | male | 30.360 | 0 | yes | southeast | 62592.87309 | obese | 1 | . 1301 62 | male | 30.875 | 3 | yes | northwest | 46718.16325 | obese | 1 | . 1308 25 | female | 30.200 | 0 | yes | southwest | 33900.65300 | obese | 1 | . 1313 19 | female | 34.700 | 2 | yes | southwest | 36397.57600 | obese | 1 | . 1323 42 | female | 40.370 | 2 | yes | southeast | 43896.37630 | obese | 1 | . 145 rows × 9 columns . df.head() . age sex bmi children smoker region charges personality obese_smoker . 0 19 | female | 27.900 | 0 | yes | southwest | 16884.92400 | overweight | 0 | . 1 18 | male | 33.770 | 1 | no | southeast | 1725.55230 | obese | 0 | . 2 28 | male | 33.000 | 3 | no | southeast | 4449.46200 | obese | 0 | . 3 33 | male | 22.705 | 0 | no | northwest | 21984.47061 | normal | 0 | . 4 32 | male | 28.880 | 0 | no | northwest | 3866.85520 | overweight | 0 | . plt.figure(figsize=(8,6),dpi=120) sns.scatterplot(data=df,x=&#39;bmi&#39;,y=&#39;charges&#39;,hue=&#39;personality&#39;) . &lt;AxesSubplot:xlabel=&#39;bmi&#39;, ylabel=&#39;charges&#39;&gt; . plt.figure(figsize=(8,6),dpi=120) sns.scatterplot(data=df,x=&#39;bmi&#39;,y=&#39;charges&#39;,hue=&#39;obese_smoker&#39;) . &lt;AxesSubplot:xlabel=&#39;bmi&#39;, ylabel=&#39;charges&#39;&gt; . Feature Engineering . df.head() . age sex bmi children smoker region charges personality obese_smoker . 0 19 | female | 27.900 | 0 | yes | southwest | 16884.92400 | overweight | 0 | . 1 18 | male | 33.770 | 1 | no | southeast | 1725.55230 | obese | 0 | . 2 28 | male | 33.000 | 3 | no | southeast | 4449.46200 | obese | 0 | . 3 33 | male | 22.705 | 0 | no | northwest | 21984.47061 | normal | 0 | . 4 32 | male | 28.880 | 0 | no | northwest | 3866.85520 | overweight | 0 | . df[&#39;children&#39;] = df[&#39;children&#39;].astype(str) . def child(number): if (number == &#39;4&#39; or number == &#39;5&#39;): return &#39;3+&#39; else: return number df[&#39;children&#39;] = df[&#39;children&#39;].apply(child) . plt.figure(figsize=(10,6),dpi=120) sns.countplot(data=df, x=&#39;children&#39;) . &lt;AxesSubplot:xlabel=&#39;children&#39;, ylabel=&#39;count&#39;&gt; . df.head() . age sex bmi children smoker region charges personality obese_smoker . 0 19 | female | 27.900 | 0 | yes | southwest | 16884.92400 | overweight | 0 | . 1 18 | male | 33.770 | 1 | no | southeast | 1725.55230 | obese | 0 | . 2 28 | male | 33.000 | 3 | no | southeast | 4449.46200 | obese | 0 | . 3 33 | male | 22.705 | 0 | no | northwest | 21984.47061 | normal | 0 | . 4 32 | male | 28.880 | 0 | no | northwest | 3866.85520 | overweight | 0 | . plt.figure(figsize=(8,4),dpi=120) sns.boxplot(data=df,x=&#39;bmi&#39;) . &lt;AxesSubplot:xlabel=&#39;bmi&#39;&gt; . df[df[&#39;bmi&#39;]&gt;50] . age sex bmi children smoker region charges personality obese_smoker . 847 23 | male | 50.38 | 1 | no | southeast | 2438.0552 | obese | 0 | . 1047 22 | male | 52.58 | 1 | yes | southeast | 44501.3982 | obese | 1 | . 1317 18 | male | 53.13 | 0 | no | southeast | 1163.4627 | obese | 0 | . df.drop([847,1047,1317],axis=0,inplace=True) . plt.figure(figsize=(8,4),dpi=120) sns.boxplot(data=df,x=&#39;bmi&#39;) . &lt;AxesSubplot:xlabel=&#39;bmi&#39;&gt; . df[df[&#39;bmi&#39;]&gt;47] . age sex bmi children smoker region charges personality obese_smoker . 116 58 | male | 49.06 | 0 | no | southeast | 11381.32540 | obese | 0 | . 286 46 | female | 48.07 | 2 | no | northeast | 9432.92530 | obese | 0 | . 401 47 | male | 47.52 | 1 | no | southeast | 8083.91980 | obese | 0 | . 543 54 | female | 47.41 | 0 | yes | southeast | 63770.42801 | obese | 1 | . 860 37 | female | 47.60 | 2 | yes | southwest | 46113.51100 | obese | 1 | . 1088 52 | male | 47.74 | 1 | no | southeast | 9748.91060 | obese | 0 | . df.drop([116,286,401,543,860,1088],axis=0,inplace=True) . plt.figure(figsize=(8,4),dpi=120) sns.boxplot(data=df,x=&#39;bmi&#39;) . &lt;AxesSubplot:xlabel=&#39;bmi&#39;&gt; . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 1329 entries, 0 to 1337 Data columns (total 9 columns): # Column Non-Null Count Dtype -- -- 0 age 1329 non-null int64 1 sex 1329 non-null object 2 bmi 1329 non-null float64 3 children 1329 non-null object 4 smoker 1329 non-null object 5 region 1329 non-null object 6 charges 1329 non-null float64 7 personality 1312 non-null object 8 obese_smoker 1329 non-null int64 dtypes: float64(2), int64(2), object(5) memory usage: 136.1+ KB . from feature_engine.encoding import OneHotEncoder encoder = OneHotEncoder(variables=[&#39;sex&#39;,&#39;children&#39;,&#39;smoker&#39;,&#39;region&#39;,]) df = encoder.fit_transform(df) . X = df.drop(&#39;charges&#39;,axis=1) y = df[&#39;charges&#39;] . from sklearn.model_selection import train_test_split X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=101,shuffle=True) . X_train[&#39;personality&#39;].fillna(&#39;overweight&#39;,inplace=True) . C: Users HP Downloads Anaconda_new lib site-packages pandas core generic.py:6392: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy return self._update_inplace(result) . X_test[&#39;personality&#39;].fillna(&#39;overweight&#39;,inplace=True) . C: Users HP Downloads Anaconda_new lib site-packages pandas core generic.py:6392: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy return self._update_inplace(result) . X_train.isnull().sum() . age 0 bmi 0 personality 0 obese_smoker 0 sex_female 0 sex_male 0 children_0 0 children_1 0 children_3 0 children_2 0 children_3+ 0 smoker_yes 0 smoker_no 0 region_southwest 0 region_southeast 0 region_northwest 0 region_northeast 0 dtype: int64 . from feature_engine.encoding import OrdinalEncoder ordinal_encoder = OrdinalEncoder(&#39;ordered&#39;,variables=[&#39;personality&#39;]) X_train = ordinal_encoder.fit_transform(X_train,y_train) X_test = ordinal_encoder.transform(X_test) . ordinal_encoder.encoder_dict_ . {&#39;personality&#39;: {&#39;underweight&#39;: 0, &#39;normal&#39;: 1, &#39;overweight&#39;: 2, &#39;obese&#39;: 3}} . Model Selection . from sklearn.linear_model import LinearRegression, ElasticNet from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import RandomForestRegressor from sklearn.svm import SVR from sklearn.ensemble import AdaBoostRegressor from sklearn.ensemble import GradientBoostingRegressor from sklearn.model_selection import GridSearchCV . from sklearn.preprocessing import StandardScaler from sklearn.pipeline import Pipeline from sklearn.metrics import mean_absolute_error, r2_score from sklearn.model_selection import cross_val_score . scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) . lr_model = LinearRegression() dt_model = DecisionTreeRegressor() rf_model = RandomForestRegressor() svm_model = SVR() adaboost_model = AdaBoostRegressor() gradientboost_model = GradientBoostingRegressor() elastic_net = ElasticNet() . models = [lr_model,dt_model,rf_model,svm_model,adaboost_model,gradientboost_model,elastic_net] . for i in models: i.fit(X_train,y_train) pred = i.predict(X_test) print(i,r2_score(y_test,pred)) . LinearRegression() 0.8575981747551504 DecisionTreeRegressor() 0.6927026516049192 RandomForestRegressor() 0.8348691937436138 SVR() -0.09323108313765038 AdaBoostRegressor() 0.7935061017123721 GradientBoostingRegressor() 0.8452379705187539 ElasticNet() 0.8207939329537015 . Metrics . for i in models: cv_score = cross_val_score(i,X_train,y_train,cv=10,scoring=&#39;r2&#39;) print(&quot;%s: %f &quot; % (i, cv_score.mean())) . LinearRegression(): 0.853135 DecisionTreeRegressor(): 0.707465 RandomForestRegressor(): 0.832795 SVR(): -0.105989 AdaBoostRegressor(): 0.768739 GradientBoostingRegressor(): 0.851339 ElasticNet(): 0.815726 . linear_pred = lr_model.predict(X_test) r2 = r2_score(y_test,linear_pred) . r2 . 0.8575981747551504 . mae = mean_absolute_error(y_test,linear_pred) . rmse = np.sqrt(mean_absolute_error(y_test,pred)) . Results = [[r2],[mae],[rmse]] . Result = pd.DataFrame(data=Results,index=[&#39;R2&#39;,&#39;MAE&#39;,&#39;RMSE&#39;],columns=[&#39;Scores&#39;]) . Result . Scores . R2 0.857598 | . MAE 2484.152515 | . RMSE 59.230400 | .",
            "url": "https://bilalkhan18.github.io/port/2022/09/12/Medical-Insurance.html",
            "relUrl": "/2022/09/12/Medical-Insurance.html",
            "date": " • Sep 12, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Topic Modelling for quora questions",
            "content": "import numpy as np import pandas as pd . df = pd.read_csv(&#39;C: Users HP OneDrive Desktop UPDATED_NLP_COURSE 05-Topic-Modeling quora_questions.csv&#39;) . df.head() . Question . 0 What is the step by step guide to invest in sh... | . 1 What is the story of Kohinoor (Koh-i-Noor) Dia... | . 2 How can I increase the speed of my internet co... | . 3 Why am I mentally very lonely? How can I solve... | . 4 Which one dissolve in water quikly sugar, salt... | . len(df) . 404289 . data = df[0:50000] . data.isnull().sum() . Question 0 dtype: int64 . from sklearn.feature_extraction.text import CountVectorizer cv = CountVectorizer(max_df = 0.90, min_df = 2, stop_words=&#39;english&#39;) dtm = cv.fit_transform(data[&#39;Question&#39;]) . dtm . &lt;50000x13573 sparse matrix of type &#39;&lt;class &#39;numpy.int64&#39;&gt;&#39; with 238154 stored elements in Compressed Sparse Row format&gt; . from sklearn.decomposition import LatentDirichletAllocation LDA = LatentDirichletAllocation(n_components = 7) LDA.fit(dtm) . LatentDirichletAllocation(n_components=7) . len(LDA.components_[0]) # Each word is treated as a feature, for which we have 50000x13573 words, and this returns the probability of each word/feature belonging to that topic . 13573 . first_topic = LDA.components_[0] . first_topic.argsort() . array([ 346, 2328, 8062, ..., 6984, 7392, 1496], dtype=int64) . first_topic[346] # Least Probability of the word belonging to first topic . 0.14285716827814351 . first_topic[1496] # Highest Probability of the word belonging to first topic . 1295.8476384240719 . for index,topic in enumerate(LDA.components_): print(f&#39;THE TOP 15 WORDS FOR TOPIC #{index}&#39;) print([cv.get_feature_names()[i] for i in topic.argsort()[-15:]]) print(&#39; n&#39;) . THE TOP 15 WORDS FOR TOPIC #0 [&#39;earn&#39;, &#39;start&#39;, &#39;ways&#39;, &#39;programming&#39;, &#39;lose&#39;, &#39;language&#39;, &#39;use&#39;, &#39;weight&#39;, &#39;india&#39;, &#39;way&#39;, &#39;online&#39;, &#39;money&#39;, &#39;learn&#39;, &#39;make&#39;, &#39;best&#39;] THE TOP 15 WORDS FOR TOPIC #1 [&#39;did&#39;, &#39;people&#39;, &#39;black&#39;, &#39;math&#39;, &#39;rs&#39;, &#39;improve&#39;, &#39;war&#39;, &#39;does&#39;, &#39;notes&#39;, &#39;1000&#39;, &#39;love&#39;, &#39;500&#39;, &#39;indian&#39;, &#39;india&#39;, &#39;world&#39;] THE TOP 15 WORDS FOR TOPIC #2 [&#39;did&#39;, &#39;women&#39;, &#39;make&#39;, &#39;used&#39;, &#39;word&#39;, &#39;sex&#39;, &#39;ask&#39;, &#39;like&#39;, &#39;examples&#39;, &#39;thing&#39;, &#39;questions&#39;, &#39;does&#39;, &#39;life&#39;, &#39;people&#39;, &#39;quora&#39;] THE TOP 15 WORDS FOR TOPIC #3 [&#39;hillary&#39;, &#39;iphone&#39;, &#39;did&#39;, &#39;movies&#39;, &#39;new&#39;, &#39;clinton&#39;, &#39;english&#39;, &#39;movie&#39;, &#39;instagram&#39;, &#39;president&#39;, &#39;donald&#39;, &#39;does&#39;, &#39;best&#39;, &#39;know&#39;, &#39;trump&#39;] THE TOP 15 WORDS FOR TOPIC #4 [&#39;quora&#39;, &#39;does&#39;, &#39;study&#39;, &#39;exam&#39;, &#39;science&#39;, &#39;computer&#39;, &#39;prepare&#39;, &#39;data&#39;, &#39;way&#39;, &#39;book&#39;, &#39;books&#39;, &#39;engineering&#39;, &#39;difference&#39;, &#39;good&#39;, &#39;best&#39;] THE TOP 15 WORDS FOR TOPIC #5 [&#39;com&#39;, &#39;india&#39;, &#39;phone&#39;, &#39;app&#39;, &#39;android&#39;, &#39;time&#39;, &#39;think&#39;, &#39;feel&#39;, &#39;best&#39;, &#39;account&#39;, &#39;facebook&#39;, &#39;stop&#39;, &#39;girl&#39;, &#39;does&#39;, &#39;like&#39;] THE TOP 15 WORDS FOR TOPIC #6 [&#39;differences&#39;, &#39;school&#39;, &#39;good&#39;, &#39;years&#39;, &#39;card&#39;, &#39;old&#39;, &#39;like&#39;, &#39;10&#39;, &#39;work&#39;, &#39;india&#39;, &#39;new&#39;, &#39;job&#39;, &#39;year&#39;, &#39;mean&#39;, &#39;does&#39;] . topic_results = LDA.transform(dtm) . topic_results[0] # Probability of each document belonging to each topic . array([0.76219992, 0.01787775, 0.14841885, 0.0178632 , 0.01788413, 0.01786298, 0.01789317]) . data[&#39;Topic&#39;] = topic_results.argmax(axis=1) . &lt;ipython-input-46-f2e4f8028d9d&gt;:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy data[&#39;Topic&#39;] = topic_results.argmax(axis=1) . data.head() . Question Topic . 0 What is the step by step guide to invest in sh... | 0 | . 1 What is the story of Kohinoor (Koh-i-Noor) Dia... | 5 | . 2 How can I increase the speed of my internet co... | 5 | . 3 Why am I mentally very lonely? How can I solve... | 2 | . 4 Which one dissolve in water quikly sugar, salt... | 2 | .",
            "url": "https://bilalkhan18.github.io/port/2022/09/05/TopicModelling.html",
            "relUrl": "/2022/09/05/TopicModelling.html",
            "date": " • Sep 5, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Review Analysis using Sentiment Analysis",
            "content": "import numpy as np import pandas as pd . df = pd.read_csv(&#39;Restaurant_Reviews.tsv&#39;, sep = &#39; t&#39;) . df.head() . Review Liked . 0 Wow... Loved this place. | 1 | . 1 Crust is not good. | 0 | . 2 Not tasty and the texture was just nasty. | 0 | . 3 Stopped by during the late May bank holiday of... | 1 | . 4 The selection on the menu was great and so wer... | 1 | . df.isnull().sum() . Review 0 Liked 0 dtype: int64 . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1000 entries, 0 to 999 Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 Review 1000 non-null object 1 Liked 1000 non-null int64 dtypes: int64(1), object(1) memory usage: 15.8+ KB . empty = [] for index,Review,Liked in df.itertuples(): if type(Review) == str: if Review.isspace(): empty.append(index) . empty . [] . from nltk.sentiment.vader import SentimentIntensityAnalyzer model = SentimentIntensityAnalyzer() df[&#39;Score&#39;] = df[&#39;Review&#39;].apply(lambda text: model.polarity_scores(text)) df.head() . Review Liked Score . 0 Wow... Loved this place. | 1 | {&#39;neg&#39;: 0.0, &#39;neu&#39;: 0.435, &#39;pos&#39;: 0.565, &#39;comp... | . 1 Crust is not good. | 0 | {&#39;neg&#39;: 0.445, &#39;neu&#39;: 0.555, &#39;pos&#39;: 0.0, &#39;comp... | . 2 Not tasty and the texture was just nasty. | 0 | {&#39;neg&#39;: 0.34, &#39;neu&#39;: 0.66, &#39;pos&#39;: 0.0, &#39;compou... | . 3 Stopped by during the late May bank holiday of... | 1 | {&#39;neg&#39;: 0.093, &#39;neu&#39;: 0.585, &#39;pos&#39;: 0.322, &#39;co... | . 4 The selection on the menu was great and so wer... | 1 | {&#39;neg&#39;: 0.0, &#39;neu&#39;: 0.728, &#39;pos&#39;: 0.272, &#39;comp... | . df[&#39;Compound&#39;] = df[&#39;Score&#39;].apply(lambda text: text[&#39;compound&#39;]) . df.head() . Review Liked Score Compound . 0 Wow... Loved this place. | 1 | {&#39;neg&#39;: 0.0, &#39;neu&#39;: 0.435, &#39;pos&#39;: 0.565, &#39;comp... | 0.5994 | . 1 Crust is not good. | 0 | {&#39;neg&#39;: 0.445, &#39;neu&#39;: 0.555, &#39;pos&#39;: 0.0, &#39;comp... | -0.3412 | . 2 Not tasty and the texture was just nasty. | 0 | {&#39;neg&#39;: 0.34, &#39;neu&#39;: 0.66, &#39;pos&#39;: 0.0, &#39;compou... | -0.5574 | . 3 Stopped by during the late May bank holiday of... | 1 | {&#39;neg&#39;: 0.093, &#39;neu&#39;: 0.585, &#39;pos&#39;: 0.322, &#39;co... | 0.6908 | . 4 The selection on the menu was great and so wer... | 1 | {&#39;neg&#39;: 0.0, &#39;neu&#39;: 0.728, &#39;pos&#39;: 0.272, &#39;comp... | 0.6249 | . def polarity(compound): if compound &gt; 0.0: return 1 else: return 0 df[&#39;Prediction&#39;] = df[&#39;Compound&#39;].apply(polarity) . df.head() . Review Liked Score Compound Prediction . 0 Wow... Loved this place. | 1 | {&#39;neg&#39;: 0.0, &#39;neu&#39;: 0.435, &#39;pos&#39;: 0.565, &#39;comp... | 0.5994 | 1 | . 1 Crust is not good. | 0 | {&#39;neg&#39;: 0.445, &#39;neu&#39;: 0.555, &#39;pos&#39;: 0.0, &#39;comp... | -0.3412 | 0 | . 2 Not tasty and the texture was just nasty. | 0 | {&#39;neg&#39;: 0.34, &#39;neu&#39;: 0.66, &#39;pos&#39;: 0.0, &#39;compou... | -0.5574 | 0 | . 3 Stopped by during the late May bank holiday of... | 1 | {&#39;neg&#39;: 0.093, &#39;neu&#39;: 0.585, &#39;pos&#39;: 0.322, &#39;co... | 0.6908 | 1 | . 4 The selection on the menu was great and so wer... | 1 | {&#39;neg&#39;: 0.0, &#39;neu&#39;: 0.728, &#39;pos&#39;: 0.272, &#39;comp... | 0.6249 | 1 | . from sklearn.metrics import classification_report, confusion_matrix print(classification_report(df[&#39;Liked&#39;], df[&#39;Prediction&#39;])) . precision recall f1-score support 0 0.81 0.81 0.81 500 1 0.81 0.81 0.81 500 accuracy 0.81 1000 macro avg 0.81 0.81 0.81 1000 weighted avg 0.81 0.81 0.81 1000 . confusion_matrix(df[&#39;Liked&#39;], df[&#39;Prediction&#39;]) . array([[406, 94], [ 94, 406]], dtype=int64) .",
            "url": "https://bilalkhan18.github.io/port/2022/09/03/ReviewAnalysis.html",
            "relUrl": "/2022/09/03/ReviewAnalysis.html",
            "date": " • Sep 3, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Loan Default Prediction",
            "content": "import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns . df = pd.read_csv(&#39;C: Users HP OneDrive Desktop Project Data Data lending_club_loan_two.csv&#39;) . df.head() . loan_amnt term int_rate installment grade sub_grade emp_title emp_length home_ownership annual_inc ... open_acc pub_rec revol_bal revol_util total_acc initial_list_status application_type mort_acc pub_rec_bankruptcies address . 0 10000.0 | 36 months | 11.44 | 329.48 | B | B4 | Marketing | 10+ years | RENT | 117000.0 | ... | 16.0 | 0.0 | 36369.0 | 41.8 | 25.0 | w | INDIVIDUAL | 0.0 | 0.0 | 0174 Michelle Gateway r nMendozaberg, OK 22690 | . 1 8000.0 | 36 months | 11.99 | 265.68 | B | B5 | Credit analyst | 4 years | MORTGAGE | 65000.0 | ... | 17.0 | 0.0 | 20131.0 | 53.3 | 27.0 | f | INDIVIDUAL | 3.0 | 0.0 | 1076 Carney Fort Apt. 347 r nLoganmouth, SD 05113 | . 2 15600.0 | 36 months | 10.49 | 506.97 | B | B3 | Statistician | &lt; 1 year | RENT | 43057.0 | ... | 13.0 | 0.0 | 11987.0 | 92.2 | 26.0 | f | INDIVIDUAL | 0.0 | 0.0 | 87025 Mark Dale Apt. 269 r nNew Sabrina, WV 05113 | . 3 7200.0 | 36 months | 6.49 | 220.65 | A | A2 | Client Advocate | 6 years | RENT | 54000.0 | ... | 6.0 | 0.0 | 5472.0 | 21.5 | 13.0 | f | INDIVIDUAL | 0.0 | 0.0 | 823 Reid Ford r nDelacruzside, MA 00813 | . 4 24375.0 | 60 months | 17.27 | 609.33 | C | C5 | Destiny Management Inc. | 9 years | MORTGAGE | 55000.0 | ... | 13.0 | 0.0 | 24584.0 | 69.8 | 43.0 | f | INDIVIDUAL | 1.0 | 0.0 | 679 Luna Roads r nGreggshire, VA 11650 | . 5 rows × 27 columns . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 396030 entries, 0 to 396029 Data columns (total 27 columns): # Column Non-Null Count Dtype -- -- 0 loan_amnt 396030 non-null float64 1 term 396030 non-null object 2 int_rate 396030 non-null float64 3 installment 396030 non-null float64 4 grade 396030 non-null object 5 sub_grade 396030 non-null object 6 emp_title 373103 non-null object 7 emp_length 377729 non-null object 8 home_ownership 396030 non-null object 9 annual_inc 396030 non-null float64 10 verification_status 396030 non-null object 11 issue_d 396030 non-null object 12 loan_status 396030 non-null object 13 purpose 396030 non-null object 14 title 394275 non-null object 15 dti 396030 non-null float64 16 earliest_cr_line 396030 non-null object 17 open_acc 396030 non-null float64 18 pub_rec 396030 non-null float64 19 revol_bal 396030 non-null float64 20 revol_util 395754 non-null float64 21 total_acc 396030 non-null float64 22 initial_list_status 396030 non-null object 23 application_type 396030 non-null object 24 mort_acc 358235 non-null float64 25 pub_rec_bankruptcies 395495 non-null float64 26 address 396030 non-null object dtypes: float64(12), object(15) memory usage: 81.6+ MB . EDA . sns.countplot(data=df,x=&#39;loan_status&#39;) . &lt;AxesSubplot:xlabel=&#39;loan_status&#39;, ylabel=&#39;count&#39;&gt; . plt.figure(figsize=(10,6),dpi=100) sns.distplot(x=df[&#39;loan_amnt&#39;],bins=40,kde=False) plt.xlabel(&#39;Loan Amount&#39;) . Text(0.5, 0, &#39;Loan Amount&#39;) . plt.figure(figsize=(10,6),dpi=120) sns.heatmap(df.corr(),cmap=&#39;viridis&#39;,annot=True) . &lt;AxesSubplot:&gt; . plt.figure(figsize=(8,6),dpi=100) sns.scatterplot(x=&#39;installment&#39;,y=&#39;loan_amnt&#39;,data=df) . &lt;AxesSubplot:xlabel=&#39;installment&#39;, ylabel=&#39;loan_amnt&#39;&gt; . plt.figure(figsize=(8,6),dpi=100) sns.boxplot(x=&#39;loan_status&#39;,y=&#39;loan_amnt&#39;,data=df) . &lt;AxesSubplot:xlabel=&#39;loan_status&#39;, ylabel=&#39;loan_amnt&#39;&gt; . df.groupby(&#39;loan_status&#39;)[&#39;loan_amnt&#39;].describe() . count mean std min 25% 50% 75% max . loan_status . Charged Off 77673.0 | 15126.300967 | 8505.090557 | 1000.0 | 8525.0 | 14000.0 | 20000.0 | 40000.0 | . Fully Paid 318357.0 | 13866.878771 | 8302.319699 | 500.0 | 7500.0 | 12000.0 | 19225.0 | 40000.0 | . df[&#39;grade&#39;].unique() . array([&#39;B&#39;, &#39;A&#39;, &#39;C&#39;, &#39;E&#39;, &#39;D&#39;, &#39;F&#39;, &#39;G&#39;], dtype=object) . df[&#39;sub_grade&#39;].unique() . array([&#39;B4&#39;, &#39;B5&#39;, &#39;B3&#39;, &#39;A2&#39;, &#39;C5&#39;, &#39;C3&#39;, &#39;A1&#39;, &#39;B2&#39;, &#39;C1&#39;, &#39;A5&#39;, &#39;E4&#39;, &#39;A4&#39;, &#39;A3&#39;, &#39;D1&#39;, &#39;C2&#39;, &#39;B1&#39;, &#39;D3&#39;, &#39;D5&#39;, &#39;D2&#39;, &#39;E1&#39;, &#39;E2&#39;, &#39;E5&#39;, &#39;F4&#39;, &#39;E3&#39;, &#39;D4&#39;, &#39;G1&#39;, &#39;F5&#39;, &#39;G2&#39;, &#39;C4&#39;, &#39;F1&#39;, &#39;F3&#39;, &#39;G5&#39;, &#39;G4&#39;, &#39;F2&#39;, &#39;G3&#39;], dtype=object) . plt.figure(figsize=(6,4),dpi=100) sns.countplot(x=&#39;grade&#39;,data=df,hue=&#39;loan_status&#39;) . &lt;AxesSubplot:xlabel=&#39;grade&#39;, ylabel=&#39;count&#39;&gt; . plt.figure(figsize=(12,4)) subgrade_order = sorted(df[&#39;sub_grade&#39;].unique()) sns.countplot(x=&#39;sub_grade&#39;,data=df,order = subgrade_order,palette=&#39;coolwarm&#39; ,hue=&#39;loan_status&#39;) . &lt;AxesSubplot:xlabel=&#39;sub_grade&#39;, ylabel=&#39;count&#39;&gt; . df[&#39;loan_status&#39;].unique() . array([&#39;Fully Paid&#39;, &#39;Charged Off&#39;], dtype=object) . df[&#39;Loan_Repaid&#39;] = df[&#39;loan_status&#39;].map({&#39;Fully Paid&#39;:1,&#39;Charged Off&#39;:0}) . df[[&#39;Loan_Repaid&#39;,&#39;loan_status&#39;]] . Loan_Repaid loan_status . 0 1 | Fully Paid | . 1 1 | Fully Paid | . 2 1 | Fully Paid | . 3 1 | Fully Paid | . 4 0 | Charged Off | . ... ... | ... | . 396025 1 | Fully Paid | . 396026 1 | Fully Paid | . 396027 1 | Fully Paid | . 396028 1 | Fully Paid | . 396029 1 | Fully Paid | . 396030 rows × 2 columns . df.corr()[&#39;Loan_Repaid&#39;].sort_values().drop(&#39;Loan_Repaid&#39;).plot(kind=&#39;bar&#39;) . &lt;AxesSubplot:&gt; . df.isnull().sum() . loan_amnt 0 term 0 int_rate 0 installment 0 grade 0 sub_grade 0 emp_title 22927 emp_length 18301 home_ownership 0 annual_inc 0 verification_status 0 issue_d 0 loan_status 0 purpose 0 title 1755 dti 0 earliest_cr_line 0 open_acc 0 pub_rec 0 revol_bal 0 revol_util 276 total_acc 0 initial_list_status 0 application_type 0 mort_acc 37795 pub_rec_bankruptcies 535 address 0 Loan_Repaid 0 dtype: int64 . df[&#39;emp_title&#39;].unique() . array([&#39;Marketing&#39;, &#39;Credit analyst &#39;, &#39;Statistician&#39;, ..., &#34;Michael&#39;s Arts &amp; Crafts&#34;, &#39;licensed bankere&#39;, &#39;Gracon Services, Inc&#39;], dtype=object) . len(df[&#39;emp_title&#39;].unique()) . 173106 . df.isnull().mean() . loan_amnt 0.000000 term 0.000000 int_rate 0.000000 installment 0.000000 grade 0.000000 sub_grade 0.000000 emp_title 0.057892 emp_length 0.046211 home_ownership 0.000000 annual_inc 0.000000 verification_status 0.000000 issue_d 0.000000 loan_status 0.000000 purpose 0.000000 title 0.004431 dti 0.000000 earliest_cr_line 0.000000 open_acc 0.000000 pub_rec 0.000000 revol_bal 0.000000 revol_util 0.000697 total_acc 0.000000 initial_list_status 0.000000 application_type 0.000000 mort_acc 0.095435 pub_rec_bankruptcies 0.001351 address 0.000000 Loan_Repaid 0.000000 dtype: float64 . df = df.drop(&#39;emp_title&#39;,axis=1) . df[&#39;emp_length&#39;].unique() . array([&#39;10+ years&#39;, &#39;4 years&#39;, &#39;&lt; 1 year&#39;, &#39;6 years&#39;, &#39;9 years&#39;, &#39;2 years&#39;, &#39;3 years&#39;, &#39;8 years&#39;, &#39;7 years&#39;, &#39;5 years&#39;, &#39;1 year&#39;, nan], dtype=object) . plt.figure(figsize=(8,4),dpi=120) sns.countplot(data=df,x=&#39;emp_length&#39;,hue=&#39;loan_status&#39;) plt.xticks(rotation=90); . sorted_order = [ &#39;&lt; 1 year&#39;, &#39;2 years&#39;,&#39;3 years&#39;,&#39;4 years&#39;,&#39;5 years&#39;,&#39;6 years&#39;, &#39;7 years&#39;,&#39;8 years&#39;,&#39;9 years&#39;,&#39;10+ years&#39;] . plt.figure(figsize=(8,4),dpi=120) sns.countplot(data=df,x=&#39;emp_length&#39;,hue=&#39;loan_status&#39;,order=sorted_order) plt.xticks(rotation=90); . Feature Engineering . emp_co = df[df[&#39;loan_status&#39;]==&quot;Charged Off&quot;].groupby(&quot;emp_length&quot;).count()[&#39;loan_status&#39;] . emp_fp = df[df[&#39;loan_status&#39;]==&quot;Fully Paid&quot;].groupby(&quot;emp_length&quot;).count()[&#39;loan_status&#39;] . emp_len = emp_co/emp_fp . emp_len . emp_length 1 year 0.248649 10+ years 0.225770 2 years 0.239560 3 years 0.242593 4 years 0.238213 5 years 0.237911 6 years 0.233341 7 years 0.241887 8 years 0.249625 9 years 0.250735 &lt; 1 year 0.260830 Name: loan_status, dtype: float64 . df = df.drop(&#39;emp_length&#39;,axis=1) . df.isnull().sum() . loan_amnt 0 term 0 int_rate 0 installment 0 grade 0 sub_grade 0 home_ownership 0 annual_inc 0 verification_status 0 issue_d 0 loan_status 0 purpose 0 title 1755 dti 0 earliest_cr_line 0 open_acc 0 pub_rec 0 revol_bal 0 revol_util 276 total_acc 0 initial_list_status 0 application_type 0 mort_acc 37795 pub_rec_bankruptcies 535 address 0 Loan_Repaid 0 dtype: int64 . len(df[&#39;title&#39;].unique()) . 48818 . df[&#39;title&#39;].head() . 0 Vacation 1 Debt consolidation 2 Credit card refinancing 3 Credit card refinancing 4 Credit Card Refinance Name: title, dtype: object . df[&#39;purpose&#39;].head() . 0 vacation 1 debt_consolidation 2 credit_card 3 credit_card 4 credit_card Name: purpose, dtype: object . df = df.drop(&#39;title&#39;,axis=1) . df.isnull().mean() . loan_amnt 0.000000 term 0.000000 int_rate 0.000000 installment 0.000000 grade 0.000000 sub_grade 0.000000 home_ownership 0.000000 annual_inc 0.000000 verification_status 0.000000 issue_d 0.000000 loan_status 0.000000 purpose 0.000000 dti 0.000000 earliest_cr_line 0.000000 open_acc 0.000000 pub_rec 0.000000 revol_bal 0.000000 revol_util 0.000697 total_acc 0.000000 initial_list_status 0.000000 application_type 0.000000 mort_acc 0.095435 pub_rec_bankruptcies 0.001351 address 0.000000 Loan_Repaid 0.000000 dtype: float64 . df.corr()[&#39;mort_acc&#39;].sort_values() . int_rate -0.082583 dti -0.025439 revol_util 0.007514 pub_rec 0.011552 pub_rec_bankruptcies 0.027239 Loan_Repaid 0.073111 open_acc 0.109205 installment 0.193694 revol_bal 0.194925 loan_amnt 0.222315 annual_inc 0.236320 total_acc 0.381072 mort_acc 1.000000 Name: mort_acc, dtype: float64 . df[&#39;mort_acc&#39;].mean() . 1.8139908160844138 . from feature_engine.imputation import MeanMedianImputer mean_imputer = MeanMedianImputer(imputation_method=&#39;mean&#39;,variables=[&#39;mort_acc&#39;]) . mean_imputer.fit(df) . MeanMedianImputer(imputation_method=&#39;mean&#39;, variables=[&#39;mort_acc&#39;]) . df = mean_imputer.transform(df) . df.isnull().mean() . loan_amnt 0.000000 term 0.000000 int_rate 0.000000 installment 0.000000 grade 0.000000 sub_grade 0.000000 home_ownership 0.000000 annual_inc 0.000000 verification_status 0.000000 issue_d 0.000000 loan_status 0.000000 purpose 0.000000 dti 0.000000 earliest_cr_line 0.000000 open_acc 0.000000 pub_rec 0.000000 revol_bal 0.000000 revol_util 0.000697 total_acc 0.000000 initial_list_status 0.000000 application_type 0.000000 mort_acc 0.000000 pub_rec_bankruptcies 0.001351 address 0.000000 Loan_Repaid 0.000000 dtype: float64 . mean_imputer.imputer_dict_ . {&#39;mort_acc&#39;: 1.8139908160844138} . df = df.dropna() . df.isnull().mean() . loan_amnt 0.0 term 0.0 int_rate 0.0 installment 0.0 grade 0.0 sub_grade 0.0 home_ownership 0.0 annual_inc 0.0 verification_status 0.0 issue_d 0.0 loan_status 0.0 purpose 0.0 dti 0.0 earliest_cr_line 0.0 open_acc 0.0 pub_rec 0.0 revol_bal 0.0 revol_util 0.0 total_acc 0.0 initial_list_status 0.0 application_type 0.0 mort_acc 0.0 pub_rec_bankruptcies 0.0 address 0.0 Loan_Repaid 0.0 dtype: float64 . df.head() . loan_amnt term int_rate installment grade sub_grade home_ownership annual_inc verification_status issue_d ... pub_rec revol_bal revol_util total_acc initial_list_status application_type mort_acc pub_rec_bankruptcies address Loan_Repaid . 0 10000.0 | 36 months | 11.44 | 329.48 | B | B4 | RENT | 117000.0 | Not Verified | Jan-2015 | ... | 0.0 | 36369.0 | 41.8 | 25.0 | w | INDIVIDUAL | 0.0 | 0.0 | 0174 Michelle Gateway r nMendozaberg, OK 22690 | 1 | . 1 8000.0 | 36 months | 11.99 | 265.68 | B | B5 | MORTGAGE | 65000.0 | Not Verified | Jan-2015 | ... | 0.0 | 20131.0 | 53.3 | 27.0 | f | INDIVIDUAL | 3.0 | 0.0 | 1076 Carney Fort Apt. 347 r nLoganmouth, SD 05113 | 1 | . 2 15600.0 | 36 months | 10.49 | 506.97 | B | B3 | RENT | 43057.0 | Source Verified | Jan-2015 | ... | 0.0 | 11987.0 | 92.2 | 26.0 | f | INDIVIDUAL | 0.0 | 0.0 | 87025 Mark Dale Apt. 269 r nNew Sabrina, WV 05113 | 1 | . 3 7200.0 | 36 months | 6.49 | 220.65 | A | A2 | RENT | 54000.0 | Not Verified | Nov-2014 | ... | 0.0 | 5472.0 | 21.5 | 13.0 | f | INDIVIDUAL | 0.0 | 0.0 | 823 Reid Ford r nDelacruzside, MA 00813 | 1 | . 4 24375.0 | 60 months | 17.27 | 609.33 | C | C5 | MORTGAGE | 55000.0 | Verified | Apr-2013 | ... | 0.0 | 24584.0 | 69.8 | 43.0 | f | INDIVIDUAL | 1.0 | 0.0 | 679 Luna Roads r nGreggshire, VA 11650 | 0 | . 5 rows × 25 columns . df.select_dtypes([&#39;object&#39;]).columns . Index([&#39;term&#39;, &#39;grade&#39;, &#39;sub_grade&#39;, &#39;home_ownership&#39;, &#39;verification_status&#39;, &#39;issue_d&#39;, &#39;loan_status&#39;, &#39;purpose&#39;, &#39;earliest_cr_line&#39;, &#39;initial_list_status&#39;, &#39;application_type&#39;, &#39;address&#39;], dtype=&#39;object&#39;) . df[&#39;term&#39;].unique() . array([&#39; 36 months&#39;, &#39; 60 months&#39;], dtype=object) . term = &#39; 36 months&#39; term[:3] . &#39; 36&#39; . df[&#39;term&#39;] = df[&#39;term&#39;].apply(lambda term: int(term[:3])) . C: Users HP AppData Local Temp ipykernel_1288 2179920926.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy df[&#39;term&#39;] = df[&#39;term&#39;].apply(lambda term: int(term[:3])) . df[&#39;term&#39;] . 0 36 1 36 2 36 3 36 4 60 .. 396025 60 396026 36 396027 36 396028 60 396029 36 Name: term, Length: 395219, dtype: int64 . df = df.drop(&#39;grade&#39;,axis=1) . df[&#39;home_ownership&#39;].value_counts() . MORTGAGE 198022 RENT 159395 OWN 37660 OTHER 110 NONE 29 ANY 3 Name: home_ownership, dtype: int64 . df[&#39;home_ownership&#39;] = df[&#39;home_ownership&#39;].replace([&#39;NONE&#39;,&#39;ANY&#39;],&#39;OTHER&#39;) . df[&#39;home_ownership&#39;].value_counts() . MORTGAGE 198022 RENT 159395 OWN 37660 OTHER 142 Name: home_ownership, dtype: int64 . df[&#39;address&#39;].value_counts() . USCGC Smith r nFPO AE 70466 8 USS Smith r nFPO AP 70466 8 USS Johnson r nFPO AE 48052 8 USNS Johnson r nFPO AE 05113 8 USNS Johnson r nFPO AP 48052 7 .. 43570 Maxwell Field Apt. 502 r nEast John, NH 22690 1 9983 Turner Cove r nSouth Gregmouth, WV 70466 1 1312 Cody Shoal r nRalphfurt, CO 29597 1 3841 Liu Crest r nClarkchester, NC 29597 1 787 Michelle Causeway r nBriannaton, AR 48052 1 Name: address, Length: 392898, dtype: int64 . df[&#39;Zip_Code&#39;] = df[&#39;address&#39;].apply(lambda ad: ad[-5:]) . df[&#39;Zip_Code&#39;] . 0 22690 1 05113 2 05113 3 00813 4 11650 ... 396025 30723 396026 05113 396027 70466 396028 29597 396029 48052 Name: Zip_Code, Length: 395219, dtype: object . df[&#39;issue_d&#39;] . 0 Jan-2015 1 Jan-2015 2 Jan-2015 3 Nov-2014 4 Apr-2013 ... 396025 Oct-2015 396026 Feb-2015 396027 Oct-2013 396028 Aug-2012 396029 Jun-2010 Name: issue_d, Length: 395219, dtype: object . df = df.drop(&#39;issue_d&#39;,axis=1) . df[&#39;earliest_cr_line&#39;] . 0 Jun-1990 1 Jul-2004 2 Aug-2007 3 Sep-2006 4 Mar-1999 ... 396025 Nov-2004 396026 Feb-2006 396027 Mar-1997 396028 Nov-1990 396029 Sep-1998 Name: earliest_cr_line, Length: 395219, dtype: object . df[&#39;earliest_cr_line&#39;] = df[&#39;earliest_cr_line&#39;].apply(lambda date: int(date[-4:])) . df[&#39;application_type&#39;].value_counts() . INDIVIDUAL 394508 JOINT 425 DIRECT_PAY 286 Name: application_type, dtype: int64 . df[&#39;purpose&#39;].value_counts() . debt_consolidation 234169 credit_card 82923 home_improvement 23961 other 21059 major_purchase 8756 small_business 5656 car 4670 medical 4175 moving 2842 vacation 2442 house 2197 wedding 1794 renewable_energy 329 educational 246 Name: purpose, dtype: int64 . df[&#39;initial_list_status&#39;].value_counts() . f 237346 w 157873 Name: initial_list_status, dtype: int64 . df.select_dtypes([&#39;object&#39;]).columns . Index([&#39;sub_grade&#39;, &#39;home_ownership&#39;, &#39;verification_status&#39;, &#39;loan_status&#39;, &#39;purpose&#39;, &#39;initial_list_status&#39;, &#39;application_type&#39;, &#39;address&#39;, &#39;Zip_Code&#39;], dtype=&#39;object&#39;) . from feature_engine.encoding import OneHotEncoder one_hot = OneHotEncoder(variables=[&#39;sub_grade&#39;, &#39;home_ownership&#39;, &#39;verification_status&#39;,&#39;purpose&#39;, &#39;initial_list_status&#39;, &#39;application_type&#39;,&#39;Zip_Code&#39;]) . one_hot.fit(df) . OneHotEncoder(variables=[&#39;sub_grade&#39;, &#39;home_ownership&#39;, &#39;verification_status&#39;, &#39;purpose&#39;, &#39;initial_list_status&#39;, &#39;application_type&#39;, &#39;Zip_Code&#39;]) . df = one_hot.transform(df) . df.select_dtypes([&#39;object&#39;]).columns . Index([&#39;loan_status&#39;, &#39;address&#39;], dtype=&#39;object&#39;) . df = df.drop(&#39;address&#39;,axis=1) . df[[&#39;loan_status&#39;,&#39;Loan_Repaid&#39;]] . loan_status Loan_Repaid . 0 Fully Paid | 1 | . 1 Fully Paid | 1 | . 2 Fully Paid | 1 | . 3 Fully Paid | 1 | . 4 Charged Off | 0 | . ... ... | ... | . 396025 Fully Paid | 1 | . 396026 Fully Paid | 1 | . 396027 Fully Paid | 1 | . 396028 Fully Paid | 1 | . 396029 Fully Paid | 1 | . 395219 rows × 2 columns . df = df.drop(&#39;loan_status&#39;,axis=1) . df.select_dtypes([&#39;object&#39;]).columns . Index([], dtype=&#39;object&#39;) . len(df) . 395219 . samp_df = df.sample(frac=0.1,random_state=101) len(samp_df) . 39522 . df = df.sample(frac=0.1,random_state=101) . Data is Ready . X = df.drop(&#39;Loan_Repaid&#39;,axis=1).values y = df[&#39;Loan_Repaid&#39;].values . X . array([[1.480e+04, 3.600e+01, 1.849e+01, ..., 0.000e+00, 0.000e+00, 0.000e+00], [3.100e+04, 6.000e+01, 1.757e+01, ..., 0.000e+00, 0.000e+00, 0.000e+00], [2.200e+04, 6.000e+01, 7.890e+00, ..., 0.000e+00, 0.000e+00, 0.000e+00], ..., [1.620e+04, 6.000e+01, 2.075e+01, ..., 0.000e+00, 0.000e+00, 0.000e+00], [2.100e+04, 6.000e+01, 2.099e+01, ..., 0.000e+00, 0.000e+00, 0.000e+00], [5.600e+03, 3.600e+01, 1.064e+01, ..., 0.000e+00, 0.000e+00, 0.000e+00]]) . y . array([1, 1, 1, ..., 0, 1, 1], dtype=int64) . from sklearn.model_selection import train_test_split X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=101) . from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler() . X_train = scaler.fit_transform(X_train) . X_test = scaler.transform(X_test) . import tensorflow as tf . from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense,Activation,Dropout from tensorflow.keras.constraints import max_norm from tensorflow.keras.callbacks import EarlyStopping . model = Sequential() model.add(Dense(78,activation=&#39;relu&#39;)) model.add(Dropout(0.2)) model.add(Dense(39,activation=&#39;relu&#39;)) model.add(Dropout(0.2)) model.add(Dense(19, activation=&#39;relu&#39;)) model.add(Dropout(0.2)) model.add(Dense(units=1,activation=&#39;sigmoid&#39;)) model.compile(loss=&#39;binary_crossentropy&#39;,optimizer=&#39;adam&#39;) . model.fit(x=X_train,y=y_train,epochs=25,batch_size=256,validation_data=(X_test,y_test),callbacks=EarlyStopping(patience=3)) . Epoch 1/25 109/109 [==============================] - 5s 11ms/step - loss: 0.4525 - val_loss: 0.3353 Epoch 2/25 109/109 [==============================] - 1s 5ms/step - loss: 0.3234 - val_loss: 0.2844 Epoch 3/25 109/109 [==============================] - 1s 5ms/step - loss: 0.2845 - val_loss: 0.2752 Epoch 4/25 109/109 [==============================] - 1s 5ms/step - loss: 0.2753 - val_loss: 0.2739 Epoch 5/25 109/109 [==============================] - 1s 6ms/step - loss: 0.2686 - val_loss: 0.2746 Epoch 6/25 109/109 [==============================] - 1s 5ms/step - loss: 0.2657 - val_loss: 0.2728 Epoch 7/25 109/109 [==============================] - 1s 7ms/step - loss: 0.2610 - val_loss: 0.2736 Epoch 8/25 109/109 [==============================] - 1s 6ms/step - loss: 0.2619 - val_loss: 0.2736 Epoch 9/25 109/109 [==============================] - 1s 5ms/step - loss: 0.2594 - val_loss: 0.2737 . &lt;keras.callbacks.History at 0x26411163df0&gt; . model.history.history . {&#39;loss&#39;: [0.4525328576564789, 0.3233562111854553, 0.2845454216003418, 0.2752994894981384, 0.26858246326446533, 0.26566797494888306, 0.2609650194644928, 0.26187482476234436, 0.25944578647613525], &#39;val_loss&#39;: [0.3353465795516968, 0.28443288803100586, 0.2752176523208618, 0.2739418148994446, 0.27460741996765137, 0.2727970778942108, 0.2736187279224396, 0.2735748291015625, 0.2736755311489105]} . loss = pd.DataFrame(model.history.history) . loss[[&#39;loss&#39;,&#39;val_loss&#39;]].plot() . &lt;AxesSubplot:&gt; . from sklearn.metrics import classification_report,confusion_matrix . predictions = model.predict(X_test) . predictions . array([[0.99992526], [0.67291206], [0.9999034 ], ..., [0.7083768 ], [0.9174067 ], [0.75031346]], dtype=float32) . predictions = np.round(predictions) . predictions . array([[1.], [1.], [1.], ..., [1.], [1.], [1.]], dtype=float32) . print(classification_report(y_test,predictions)) . precision recall f1-score support 0 0.97 0.42 0.58 2382 1 0.87 1.00 0.93 9475 accuracy 0.88 11857 macro avg 0.92 0.71 0.76 11857 weighted avg 0.89 0.88 0.86 11857 . confusion_matrix(y_test,predictions) . array([[ 997, 1385], [ 36, 9439]], dtype=int64) . import random random_ind = random.randint(0,len(df)) new_customer = df.drop(&#39;Loan_Repaid&#39;,axis=1).iloc[random_ind] new_customer . loan_amnt 10000.00 term 36.00 int_rate 6.49 installment 306.45 annual_inc 30000.00 ... Zip_Code_70466 0.00 Zip_Code_29597 0.00 Zip_Code_48052 0.00 Zip_Code_86630 0.00 Zip_Code_93700 0.00 Name: 359704, Length: 85, dtype: float64 . model.predict(new_customer.values.reshape(1,85)) . array([[1.]], dtype=float32) . df.iloc[random_ind][&#39;Loan_Repaid&#39;] . 1.0 . from tensorflow.keras.models import load_model . model.save(&#39;loan_default_model.h5&#39;) .",
            "url": "https://bilalkhan18.github.io/port/2022/09/01/LoanDefault.html",
            "relUrl": "/2022/09/01/LoanDefault.html",
            "date": " • Sep 1, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is Bilal Khan. I’m currently pursuing Engineering in Artificial Intelligence and Machine Learning (3rd year). Machine Learning has been exciting and this page reflects projects which are a result of self-learning. I strongly believe that artificial intelligence is a revolution and i’m keen in contributing to a better world through AI. Cheers. .",
          "url": "https://bilalkhan18.github.io/port/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://bilalkhan18.github.io/port/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}